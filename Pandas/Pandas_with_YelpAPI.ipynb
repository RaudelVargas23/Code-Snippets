{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EyBo-ReOp7T"
   },
   "source": [
    "## Part 1: Data processing with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIJ1voKlOp7U"
   },
   "source": [
    "In this section, you will see examples of some commonly used data wrangling tools in Python. In particular, we aim to give you some familiarity with:\n",
    "\n",
    "* Slicing data frames\n",
    "* Filtering data\n",
    "* Grouped counts\n",
    "* Joining two tables\n",
    "* NA/Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ha_Gdv3Op7V"
   },
   "source": [
    "\n",
    "### Practice: Pandas and Wrangling (5%)\n",
    "\n",
    "This part of the homework is graded manually based on showing the correct outputs after executing each step.\n",
    "\n",
    "You need to execute each step (run each Cell), in order for the next ones to work.\n",
    "\n",
    "For the examples that follow, we will be using a toy data set containing information about superheroes in the Arrowverse.  In the `first_seen_on` column, `a` stands for Archer and `f`, Flash.\n",
    "\n",
    "\n",
    "First, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "x02hvd8BOp7W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGJ5kCYqOp7Y"
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_VKBhnROp7Y"
   },
   "source": [
    "The code below produces the data frames used in the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "AbK1WHZmOp7Y"
   },
   "outputs": [],
   "source": [
    "heroes = pd.DataFrame(\n",
    "    data={'color': ['red', 'green', 'black',\n",
    "                    'blue', 'black', 'red'],\n",
    "          'first_seen_on': ['a', 'a', 'f', 'a', 'a', 'f'],\n",
    "          'first_season': [2, 1, 2, 3, 3, 1]},\n",
    "    index=['flash', 'arrow', 'vibe',\n",
    "           'atom', 'canary', 'firestorm']\n",
    ")\n",
    "\n",
    "identities = pd.DataFrame(\n",
    "    data={'ego': ['barry allen', 'oliver queen', 'cisco ramon',\n",
    "                  'ray palmer', 'sara lance',\n",
    "                  'martin stein', 'ronnie raymond'],\n",
    "          'alter-ego': ['flash', 'arrow', 'vibe', 'atom',\n",
    "                        'canary', 'firestorm', 'firestorm']}\n",
    ")\n",
    "\n",
    "teams = pd.DataFrame(\n",
    "    data={'team': ['flash', 'arrow', 'flash', 'legends',\n",
    "                   'flash', 'legends', 'arrow'],\n",
    "          'hero': ['flash', 'arrow', 'vibe', 'atom',\n",
    "                   'killer frost', 'firestorm', 'speedy']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "QrFDF1bWOp7Z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color first_seen_on  first_season\n",
       "flash        red             a             2\n",
       "arrow      green             a             1\n",
       "vibe       black             f             2\n",
       "atom        blue             a             3\n",
       "canary     black             a             3\n",
       "firestorm    red             f             1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "4XY_z8jLOp7Z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego</th>\n",
       "      <th>alter-ego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barry allen</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oliver queen</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cisco ramon</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ray palmer</td>\n",
       "      <td>atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara lance</td>\n",
       "      <td>canary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>martin stein</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ronnie raymond</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ego  alter-ego\n",
       "0     barry allen      flash\n",
       "1    oliver queen      arrow\n",
       "2     cisco ramon       vibe\n",
       "3      ray palmer       atom\n",
       "4      sara lance     canary\n",
       "5    martin stein  firestorm\n",
       "6  ronnie raymond  firestorm"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "WOY6B7x2Op7Z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>hero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flash</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arrow</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flash</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>legends</td>\n",
       "      <td>atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flash</td>\n",
       "      <td>killer frost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>legends</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arrow</td>\n",
       "      <td>speedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      team          hero\n",
       "0    flash         flash\n",
       "1    arrow         arrow\n",
       "2    flash          vibe\n",
       "3  legends          atom\n",
       "4    flash  killer frost\n",
       "5  legends     firestorm\n",
       "6    arrow        speedy"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drTp3n3mOp7a"
   },
   "source": [
    "#### Slice and Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvIefSxlOp7a"
   },
   "source": [
    "##### Column selection by label\n",
    "To select a column of a `DataFrame` by column label, the safest and fastest way is to use the `.loc` method. General usage looks like `frame.loc[rowname,colname]`. (Reminder that the colon `:` means \"everything\").  For example, if we want the `color` column of the `heroes` data frame, we would use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "5TwiwdlzOp7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flash          red\n",
       "arrow        green\n",
       "vibe         black\n",
       "atom          blue\n",
       "canary       black\n",
       "firestorm      red\n",
       "Name: color, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[:,'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOWuCPu0Wszt"
   },
   "source": [
    "The above output can also be got in dataframe format instead of series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "nL0ytnppWkZN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color\n",
       "flash        red\n",
       "arrow      green\n",
       "vibe       black\n",
       "atom        blue\n",
       "canary     black\n",
       "firestorm    red"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[:,['color']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbbdrZifW2ni"
   },
   "source": [
    "Another way of doing the above is shown below. The below method creates a boolean mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "sTeEb5bhWbz7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color\n",
       "flash        red\n",
       "arrow      green\n",
       "vibe       black\n",
       "atom        blue\n",
       "canary     black\n",
       "firestorm    red"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[:, heroes.columns=='color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_j0sqlHOp7a"
   },
   "source": [
    "Selecting multiple columns is easy. You just need to supply a list of column names. Here we select the color and value columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "oM6sh5xYOp7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color  first_season\n",
       "flash        red             2\n",
       "arrow      green             1\n",
       "vibe       black             2\n",
       "atom        blue             3\n",
       "canary     black             3\n",
       "firestorm    red             1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[:, ['color', 'first_season']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_Wgc_10Op7b"
   },
   "source": [
    "While .loc is invaluable when writing production code, it may be a little too verbose for interactive use. One recommended alternative is the [] method, which takes on the form frame['colname']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "K91Og1TxOp7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flash        a\n",
       "arrow        a\n",
       "vibe         f\n",
       "atom         a\n",
       "canary       a\n",
       "firestorm    f\n",
       "Name: first_seen_on, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes['first_seen_on']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1fl9GF4Op7b"
   },
   "source": [
    "##### Row Selection by Label\n",
    "\n",
    "Similarly, if we want to select a row by its label, we can use the same .loc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "YccQu_uqOp7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       color first_seen_on  first_season\n",
       "flash    red             a             2\n",
       "vibe   black             f             2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[['flash', 'vibe'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFV3q3mUOp7b"
   },
   "source": [
    "If we want all the columns returned, we can, for brevity, drop the colon without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "_u_ncVSIOp7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       color first_seen_on  first_season\n",
       "flash    red             a             2\n",
       "vibe   black             f             2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc[['flash', 'vibe']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQUkPJfHOp7c"
   },
   "source": [
    "##### General Selection by Label\n",
    "\n",
    "More generally you can slice across both rows and columns at the same time.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Tfy5BZf8Op7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       color first_seen_on\n",
       "flash    red             a\n",
       "arrow  green             a\n",
       "vibe   black             f\n",
       "atom    blue             a"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.loc['flash':'atom', :'first_seen_on']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9eASGKwOp7c"
   },
   "source": [
    "##### Selection by Integer Index\n",
    "\n",
    "If you want to select rows and columns by position, the Data Frame has an analogous `.iloc` method for integer indexing. Remember that Python indexing starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "rGPlBBayOp7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       color first_seen_on\n",
       "flash    red             a\n",
       "arrow  green             a\n",
       "vibe   black             f\n",
       "atom    blue             a"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.iloc[:4,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBZi-OWkOp7d"
   },
   "source": [
    "#### Filtering with boolean arrays\n",
    "\n",
    "Filtering is the process of removing unwanted material.  In your quest for cleaner data, you will undoubtedly filter your data at some point: whether it be for clearing up cases with missing values, culling out fishy outliers, or analyzing subgroups of your data set.  For example, we may be interested in characters that debuted in season 3 of Archer.  Note that compound expressions have to be grouped with parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "hK5Dw32QOp7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        color first_seen_on  first_season\n",
       "atom     blue             a             3\n",
       "canary  black             a             3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes[(heroes['first_season']==3) & (heroes['first_seen_on']=='a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-l5fs-FOp7d"
   },
   "source": [
    "#### Problem Solving Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bJ5dkluOp7d"
   },
   "source": [
    "We want to highlight the strategy for filtering to answer the question above:\n",
    "\n",
    "* **Identify the variables of interest**\n",
    "    * Interested in the debut: `first_season` and `first_seen_on`\n",
    "* **Translate the question into statements one with True/False answers**\n",
    "    * Did the hero debut on Archer? $\\rightarrow$ The hero has `first_seen_on` equal to `a`\n",
    "    * Did the hero debut in season 3? $\\rightarrow$ The hero has `first_season` equal to `3`\n",
    "* **Translate the statements into boolean statements**\n",
    "    * The hero has `first_seen_on` equal to `a` $\\rightarrow$ `hero['first_seen_on']=='a'`\n",
    "    * The hero has `first_season` equal to `3` $\\rightarrow$ `heroes['first_season']==3`\n",
    "* **Use the boolean array to filter the data**\n",
    "\n",
    "Note that compound expressions have to be grouped with parentheses.\n",
    "\n",
    "For your reference, some commonly used comparison operators are given below.\n",
    "\n",
    "Symbol | Usage      | Meaning\n",
    "------ | ---------- | -------------------------------------\n",
    "==   | a == b   | Does a equal b?\n",
    "<=   | a <= b   | Is a less than or equal to b?\n",
    ">=   | a >= b   | Is a greater than or equal to b?\n",
    "<    | a < b    | Is a less than b?\n",
    "&#62;    | a &#62; b    | Is a greater than b?\n",
    "~    | ~p       | Returns negation of p\n",
    "&#124; | p &#124; q | p OR q\n",
    "&    | p & q    | p AND q\n",
    "^  | p ^ q | p XOR q (exclusive or)\n",
    "\n",
    "An often-used operation missing from the above table is a test-of-membership.  The `Series.isin(values)` method returns a boolean array denoting whether each element of `Series` is in `values`.  We can then use the array to subset our data frame. For example, if we wanted to see which rows of `heroes` had values in $\\{1,3\\}$, we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "wtG6kk6iOp7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color first_seen_on  first_season\n",
       "arrow      green             a             1\n",
       "atom        blue             a             3\n",
       "canary     black             a             3\n",
       "firestorm    red             f             1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes[heroes['first_season'].isin([1,3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEC2Zx9fOp7d"
   },
   "source": [
    "Notice that in both examples above, the expression in the brackets evaluates to a boolean series.  The general strategy for filtering data frames, then, is to write an expression of the form `frame[logical statement]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aV0ZMVSOp7d"
   },
   "source": [
    "#### Counting Rows\n",
    "\n",
    "To count the number of instances of a value in a `Series`, we can use the `value_counts` method.  Below we count the number of instances of each color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Blcz7LzQOp7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "red      2\n",
       "black    2\n",
       "green    1\n",
       "blue     1\n",
       "Name: color, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes['color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU4xkoDXOp7e"
   },
   "source": [
    "A more sophisticated analysis might involve counting the number of instances a tuple appears.  Here we count $(color,value)$ tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "9Rs94Xi2Op7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color  first_season\n",
       "black  2               1\n",
       "       3               1\n",
       "blue   3               1\n",
       "green  1               1\n",
       "red    1               1\n",
       "       2               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.groupby(['color', 'first_season']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF-YrghbOp7e"
   },
   "source": [
    "This returns a series that has been multi-indexed.  We'll eschew this topic for now.  To get a data frame back, we'll use the `reset_index` method, which also allows us to simulataneously name the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "5uUBO73nOp7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_season</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>black</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  first_season  count\n",
       "0  black             2      1\n",
       "1  black             3      1\n",
       "2   blue             3      1\n",
       "3  green             1      1\n",
       "4    red             1      1\n",
       "5    red             2      1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes.groupby(['color', 'first_season']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1X68glfOp7e"
   },
   "source": [
    "#### Joining Tables on One Column\n",
    "\n",
    "Suppose we have another table that classifies superheroes into their respective teams.  Note that `canary` is not in this data set and that `killer frost` and `speedy` are additions that aren't in the original `heroes` set.\n",
    "\n",
    "For simplicity of the example, we'll convert the index of the `heroes` data frame into an explicit column called `hero`.  A careful examination of the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) will reveal that joining on a mixture of the index and columns is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "y597O0QxOp7h"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "      <th>hero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flash</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibe</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atom</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canary</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>canary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firestorm</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           color first_seen_on  first_season       hero\n",
       "flash        red             a             2      flash\n",
       "arrow      green             a             1      arrow\n",
       "vibe       black             f             2       vibe\n",
       "atom        blue             a             3       atom\n",
       "canary     black             a             3     canary\n",
       "firestorm    red             f             1  firestorm"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heroes['hero'] = heroes.index\n",
    "heroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6HlfRDKOp7h"
   },
   "source": [
    "##### Inner Join\n",
    "\n",
    "The inner join below returns rows representing the heroes that appear in both data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "liX9S3waOp7h"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "      <th>hero</th>\n",
       "      <th>team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>flash</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>arrow</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>vibe</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>atom</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>firestorm</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color first_seen_on  first_season       hero     team\n",
       "0    red             a             2      flash    flash\n",
       "1  green             a             1      arrow    arrow\n",
       "2  black             f             2       vibe    flash\n",
       "3   blue             a             3       atom  legends\n",
       "4    red             f             1  firestorm  legends"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(heroes, teams, how='inner', on='hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztmmxXLDOp7h"
   },
   "source": [
    "##### Left and right join\n",
    "The left join returns rows representing heroes in the `heroes` (\"left\") data frame, augmented by information found in the `teams` data frame.  Its counterpart, the right join, would return heroes in the `teams` data frame.  Note that the `team` for hero `canary` is an `NaN` value, representing missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "bnd3FWTvOp7i"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "      <th>hero</th>\n",
       "      <th>team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>flash</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>arrow</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>vibe</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>atom</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>canary</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>firestorm</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color first_seen_on  first_season       hero     team\n",
       "0    red             a             2      flash    flash\n",
       "1  green             a             1      arrow    arrow\n",
       "2  black             f             2       vibe    flash\n",
       "3   blue             a             3       atom  legends\n",
       "4  black             a             3     canary      NaN\n",
       "5    red             f             1  firestorm  legends"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(heroes, teams, how='left', on='hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8keEMQHOp7i"
   },
   "source": [
    "##### Outer join\n",
    "\n",
    "An outer join on `hero` will return all heroes found in both the left and right data frames.  Any missing values are filled in with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "VcHeeoZAOp7i"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "      <th>hero</th>\n",
       "      <th>team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>flash</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>arrow</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2.0</td>\n",
       "      <td>vibe</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>atom</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>canary</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>firestorm</td>\n",
       "      <td>legends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>killer frost</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>speedy</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color first_seen_on  first_season          hero     team\n",
       "0    red             a           2.0         flash    flash\n",
       "1  green             a           1.0         arrow    arrow\n",
       "2  black             f           2.0          vibe    flash\n",
       "3   blue             a           3.0          atom  legends\n",
       "4  black             a           3.0        canary      NaN\n",
       "5    red             f           1.0     firestorm  legends\n",
       "6    NaN           NaN           NaN  killer frost    flash\n",
       "7    NaN           NaN           NaN        speedy    arrow"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(heroes, teams, how='outer', on='hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1Gnpw46Op7i"
   },
   "source": [
    "##### More than one match?\n",
    "\n",
    "If the values in the columns to be matched don't uniquely identify a row, then a cartesian product is formed in the merge.  For example, notice that `firestorm` has two different egos, so information from `heroes` had to be duplicated in the merge, once for each ego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "uW5BospiOp7i"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>first_seen_on</th>\n",
       "      <th>first_season</th>\n",
       "      <th>hero</th>\n",
       "      <th>ego</th>\n",
       "      <th>alter-ego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>flash</td>\n",
       "      <td>barry allen</td>\n",
       "      <td>flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>arrow</td>\n",
       "      <td>oliver queen</td>\n",
       "      <td>arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>vibe</td>\n",
       "      <td>cisco ramon</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>atom</td>\n",
       "      <td>ray palmer</td>\n",
       "      <td>atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "      <td>canary</td>\n",
       "      <td>sara lance</td>\n",
       "      <td>canary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>firestorm</td>\n",
       "      <td>martin stein</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>red</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>firestorm</td>\n",
       "      <td>ronnie raymond</td>\n",
       "      <td>firestorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color first_seen_on  first_season       hero             ego  alter-ego\n",
       "0    red             a             2      flash     barry allen      flash\n",
       "1  green             a             1      arrow    oliver queen      arrow\n",
       "2  black             f             2       vibe     cisco ramon       vibe\n",
       "3   blue             a             3       atom      ray palmer       atom\n",
       "4  black             a             3     canary      sara lance     canary\n",
       "5    red             f             1  firestorm    martin stein  firestorm\n",
       "6    red             f             1  firestorm  ronnie raymond  firestorm"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(heroes, identities, how='inner',\n",
    "         left_on='hero', right_on='alter-ego')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot3ZZMe6Op7j"
   },
   "source": [
    "#### Missing Values\n",
    "\n",
    "There are a multitude of reasons why a data set might have missing values.  The current implementation of Pandas uses the numpy NaN to represent these null values (older implementations even used `-inf` and `inf`).  Future versions of Pandas might implement a true `null` value---keep your eyes peeled for this in updates!  More information can be found [http://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html](http://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "\n",
    "Because of the specialness of missing values, they merit their own set of tools.  Here, we will focus on detection.  For replacement, see the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Lq4dQePXOp7j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    3.0\n",
       "4    3.0\n",
       "5    1.0\n",
       "6    NaN\n",
       "7    NaN\n",
       "Name: first_season, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.nan\n",
    "y = pd.merge(heroes, teams, how='outer', on='hero')['first_season']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpQ95Hz9Op7j"
   },
   "source": [
    "To check if a value is null, we use the `isnull()` method for series and data frames.  Alternatively, there is a `pd.isnull()` function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "PsAUEg_BOp7j"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m.\u001b[39misnull()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "x.isnull() # won't work since x is neither a series nor a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "896cM3UVOp7j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "oQduBqueOp7k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6     True\n",
       "7     True\n",
       "Name: first_season, dtype: bool"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "3lsg07a0Op7k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6     True\n",
       "7     True\n",
       "Name: first_season, dtype: bool"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VnWwxWoOp7k"
   },
   "source": [
    "Since filtering out missing data is such a common operation, Pandas also has conveniently included the analogous `notnull()` methods and function for improved human readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "b3Wwy9gtOp7l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6    False\n",
       "7    False\n",
       "Name: first_season, dtype: bool"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "gLgm5Ji6Op7l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    3.0\n",
       "4    3.0\n",
       "5    1.0\n",
       "Name: first_season, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVse0isROp7l"
   },
   "source": [
    "### Questions (45%)\n",
    "\n",
    "The practice problems below use the department of transportation's \"On-Time\" flight data for all flights originating from SFO or OAK in January 2016. Information about the airports and airlines are contained in the comma-delimited files `airports.dat` and `airlines.dat`, respectively.  Both were sourced from http://openflights.org/data.html.\n",
    "\n",
    "Disclaimer: There is a more direct way of dealing with time data that is not presented in these problems.  This activity is merely an academic exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoHdTMBMOp7l"
   },
   "source": [
    "#### Setup\n",
    "Run the below cells to set up the tables flights and airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "UP8oKxJsOp7l"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>carrier</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>flight</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>actual_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>actual_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3FLAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>630.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3APAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>600.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3DNAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>630.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1431.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day        date carrier tailnum  flight origin destination  \\\n",
       "0  2016      1    1  2016-01-01      AA  N3FLAA     208    SFO         MIA   \n",
       "1  2016      1    2  2016-01-02      AA  N3APAA     208    SFO         MIA   \n",
       "2  2016      1    3  2016-01-03      AA  N3DNAA     208    SFO         MIA   \n",
       "\n",
       "   sched_dep_time  actual_dep_time  sched_arr_time  actual_arr_time  \n",
       "0           630.0            628.0          1458.0           1431.0  \n",
       "1           600.0            553.0          1428.0           1401.0  \n",
       "2           630.0            626.0          1458.0           1431.0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = pd.read_csv(\"flights.dat\", dtype={'sched_dep_time': 'f8', 'sched_arr_time': 'f8'})\n",
    "# show the first few rows, by default 5\n",
    "flights.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "he-b0pCguI_x"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>actual_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>actual_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16861.0</td>\n",
       "      <td>16861.0</td>\n",
       "      <td>16861.000000</td>\n",
       "      <td>16861.000000</td>\n",
       "      <td>16861.000000</td>\n",
       "      <td>16399.000000</td>\n",
       "      <td>16861.000000</td>\n",
       "      <td>16365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.781033</td>\n",
       "      <td>2100.205978</td>\n",
       "      <td>1333.387818</td>\n",
       "      <td>1334.757851</td>\n",
       "      <td>1518.619773</td>\n",
       "      <td>1509.387779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.978231</td>\n",
       "      <td>2036.303273</td>\n",
       "      <td>503.163806</td>\n",
       "      <td>518.007117</td>\n",
       "      <td>529.272153</td>\n",
       "      <td>548.227990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>915.000000</td>\n",
       "      <td>915.000000</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>1141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1558.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2771.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>1736.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>1948.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>6845.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          year    month           day        flight  sched_dep_time  \\\n",
       "count  16861.0  16861.0  16861.000000  16861.000000    16861.000000   \n",
       "mean    2016.0      1.0     15.781033   2100.205978     1333.387818   \n",
       "std        0.0      0.0      8.978231   2036.303273      503.163806   \n",
       "min     2016.0      1.0      1.000000      1.000000        5.000000   \n",
       "25%     2016.0      1.0      8.000000    506.000000      915.000000   \n",
       "50%     2016.0      1.0     16.000000   1372.000000     1300.000000   \n",
       "75%     2016.0      1.0     24.000000   2771.000000     1725.000000   \n",
       "max     2016.0      1.0     31.000000   6845.000000     2359.000000   \n",
       "\n",
       "       actual_dep_time  sched_arr_time  actual_arr_time  \n",
       "count     16399.000000    16861.000000     16365.000000  \n",
       "mean       1334.757851     1518.619773      1509.387779  \n",
       "std         518.007117      529.272153       548.227990  \n",
       "min           2.000000        2.000000         1.000000  \n",
       "25%         915.000000     1150.000000      1141.000000  \n",
       "50%        1309.000000     1600.000000      1558.000000  \n",
       "75%        1736.000000     1945.000000      1948.000000  \n",
       "max        2400.000000     2359.000000      2400.000000  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Dg4JU3hUOp7m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>openflights_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>iata</th>\n",
       "      <th>icao</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>altitude</th>\n",
       "      <th>tz</th>\n",
       "      <th>dst</th>\n",
       "      <th>tz_olson</th>\n",
       "      <th>type</th>\n",
       "      <th>airport_dsource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Goroka</td>\n",
       "      <td>Goroka</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>GKA</td>\n",
       "      <td>AYGA</td>\n",
       "      <td>-6.081689</td>\n",
       "      <td>145.391881</td>\n",
       "      <td>5282</td>\n",
       "      <td>10.0</td>\n",
       "      <td>U</td>\n",
       "      <td>Pacific/Port_Moresby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Madang</td>\n",
       "      <td>Madang</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>MAG</td>\n",
       "      <td>AYMD</td>\n",
       "      <td>-5.207083</td>\n",
       "      <td>145.788700</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>U</td>\n",
       "      <td>Pacific/Port_Moresby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mount Hagen</td>\n",
       "      <td>Mount Hagen</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>HGU</td>\n",
       "      <td>AYMH</td>\n",
       "      <td>-5.826789</td>\n",
       "      <td>144.295861</td>\n",
       "      <td>5388</td>\n",
       "      <td>10.0</td>\n",
       "      <td>U</td>\n",
       "      <td>Pacific/Port_Moresby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   openflights_id         name         city           country iata  icao  \\\n",
       "0               1       Goroka       Goroka  Papua New Guinea  GKA  AYGA   \n",
       "1               2       Madang       Madang  Papua New Guinea  MAG  AYMD   \n",
       "2               3  Mount Hagen  Mount Hagen  Papua New Guinea  HGU  AYMH   \n",
       "\n",
       "   latitude   longitude  altitude    tz dst              tz_olson  type  \\\n",
       "0 -6.081689  145.391881      5282  10.0   U  Pacific/Port_Moresby   NaN   \n",
       "1 -5.207083  145.788700        20  10.0   U  Pacific/Port_Moresby   NaN   \n",
       "2 -5.826789  144.295861      5388  10.0   U  Pacific/Port_Moresby   NaN   \n",
       "\n",
       "   airport_dsource  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_cols = [\n",
    "    'openflights_id',\n",
    "    'name',\n",
    "    'city',\n",
    "    'country',\n",
    "    'iata',\n",
    "    'icao',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'altitude',\n",
    "    'tz',\n",
    "    'dst',\n",
    "    'tz_olson',\n",
    "    'type',\n",
    "    'airport_dsource'\n",
    "]\n",
    "\n",
    "airports = pd.read_csv(\"airports.dat\", names=airports_cols)\n",
    "airports.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbCZ6WhrL5BP"
   },
   "source": [
    "#### Question 1.1 (15%)\n",
    "Lets practice some pandas commands in this sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Awp_uMQNPERd"
   },
   "source": [
    "(5%) In the 'flights' table, display unique values present in columns - 'carrier', 'origin', 'destination'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "EYDWLewkNC9p",
    "outputId": "3a6be2e7-e161-47d4-90d0-a521cb7098ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['AA', 'AS', 'B6', 'DL', 'F9', 'HA', 'NK', 'OO', 'UA', 'VX', 'WN'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code - write one line code to display unique values in column 'carrier'\n",
    "# write here\n",
    "out = flights['carrier'].unique()\n",
    "\n",
    "\n",
    "# testing the code < do not change code below>\n",
    "print(len(out))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LiJ37YbTP5Ad",
    "outputId": "a9121817-85d0-4b2d-cc38-7268d4bf9af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['SFO', 'OAK'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code - write one line code to display unique values in column 'carrier'\n",
    "# write here\n",
    "out = flights['origin'].unique()\n",
    "\n",
    "\n",
    "# testing the code < do not change code below>\n",
    "print(len(out))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Gk1mGhj8P-Ie",
    "outputId": "58e1c9e2-4dba-4b40-c03d-f6dee2dff76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['MIA', 'LAX', 'JFK', 'DFW', 'ORD', 'PHL', 'PHX', 'CLT', 'KOA',\n",
       "       'SEA', 'PDX', 'PSP', 'SLC', 'OGG', 'HNL', 'LIH', 'BOS', 'FLL',\n",
       "       'LGB', 'LAS', 'DTW', 'ATL', 'MSP', 'IAH', 'DEN', 'EUG', 'BOI',\n",
       "       'BUR', 'SMX', 'SAN', 'ACV', 'ASE', 'MRY', 'STL', 'SBA', 'SMF',\n",
       "       'JAC', 'SAT', 'XNA', 'AUS', 'MTJ', 'MFR', 'MCI', 'MMH', 'OKC',\n",
       "       'ONT', 'FAT', 'PSC', 'RDM', 'SBP', 'RNO', 'BFL', 'BZN', 'TUS',\n",
       "       'SNA', 'HDN', 'SUN', 'ABQ', 'EWR', 'MCO', 'MSY', 'IAD', 'IND',\n",
       "       'DCA', 'RDU', 'CLE', 'RDD', 'OTH', 'BWI', 'PIT', 'ANC', 'DAL',\n",
       "       'MDW', 'MKE', 'BNA', 'CMH', 'GEG', 'HOU', 'CVG'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code - write one line code to display unique values in column 'carrier'\n",
    "# write here\n",
    "out = flights['destination'].unique()\n",
    "\n",
    "\n",
    "# testing the code <do not change code below>\n",
    "print(len(out))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlfGjxaSQkfh"
   },
   "source": [
    "(5%) Display list of all flights with \"SFO\" as origin and \"JFK\" as destination run by the carrier \"DL\" . Only display columns - \"origin\", \"destination\" \"carrier\" and \"sched_dep_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8jh__syLL4mg",
    "outputId": "40cf519f-44c0-44d5-f437-aab0c391ac40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>carrier</th>\n",
       "      <th>sched_dep_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>2145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>615.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>1415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>1415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16783</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16787</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>2148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16791</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>2148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16812</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16813</th>\n",
       "      <td>SFO</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "      <td>830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      origin destination carrier  sched_dep_time\n",
       "2473     SFO         JFK      DL          2145.0\n",
       "2478     SFO         JFK      DL           615.0\n",
       "2479     SFO         JFK      DL          1415.0\n",
       "2480     SFO         JFK      DL           800.0\n",
       "2502     SFO         JFK      DL          1415.0\n",
       "...      ...         ...     ...             ...\n",
       "16783    SFO         JFK      DL          1600.0\n",
       "16787    SFO         JFK      DL          2148.0\n",
       "16791    SFO         JFK      DL          2148.0\n",
       "16812    SFO         JFK      DL           625.0\n",
       "16813    SFO         JFK      DL           830.0\n",
       "\n",
       "[179 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "#out = flights[['origin','destination','sched_dep_time']]\n",
    "\n",
    "out = flights.loc[(flights['origin'] == 'SFO') & (flights['destination'] == 'JFK') & (flights['carrier'] == 'DL'),['origin','destination','carrier','sched_dep_time']]\n",
    "\n",
    "\n",
    "# display\n",
    "out\n",
    "#display output like this to get full credit (179 rows):\n",
    "#     origin\tdestination\tcarrier\tsched_dep_time\n",
    "# 2473 SFO     JFK          DL      2145.0\n",
    "# 2478 SFO      JFK         DL      615.0\n",
    "# 2479 SFO      JFK         DL      1415.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcnwfRmlSkAE"
   },
   "source": [
    "(5%) - AGGREGATION - Find the total number of flights run by each carrier. Sort the list in descending order of number of flights. Just display numeric index,  carrier and number of flights. Hint: check documentationf for: groupby , sort_values, reset_index\n",
    "\n",
    "Note: Store the group in \"out\" variable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "dJc8AbR5NB3C"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carrier</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WN</td>\n",
       "      <td>4169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA</td>\n",
       "      <td>3687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OO</td>\n",
       "      <td>3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VX</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DL</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AS</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B6</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F9</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NK</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HA</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carrier  count\n",
       "0       WN   4169\n",
       "1       UA   3687\n",
       "2       OO   3325\n",
       "3       VX   1548\n",
       "4       AA   1338\n",
       "5       DL   1073\n",
       "6       AS    615\n",
       "7       B6    610\n",
       "8       F9    186\n",
       "9       NK    186\n",
       "10      HA    124"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_carrier_counts(flights):\n",
    "    #DOUBLE CHECK TO MAKE SURE IT WORKS CORRECTLY\n",
    "    \n",
    "    #group them first \n",
    "    #reset index \n",
    "    #lastly sort by descending\n",
    "    flightCounts = flights.groupby('carrier').size().reset_index(name = 'count')\n",
    "    out = flightCounts.sort_values(by = 'count',ascending = False).reset_index(drop = True)\n",
    "  \n",
    "    \n",
    "  ##======= Do not write below this === # return out should remain for full credit\n",
    "    return out\n",
    "\n",
    "# Display in below format for full credit\n",
    "#       carrier   count\n",
    "#  0    WN        4169\n",
    "#  1    UA        3687\n",
    "\n",
    "out = display_carrier_counts(flights)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "ZzIKd5hdX1MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16861"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code - RUN this code to get full credit\n",
    "# test code: Note that the sum of values should be equal to the lenght of flights table\n",
    "out[\"count\"].sum()\n",
    "# 16861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abOmZpLKOp7m"
   },
   "source": [
    "#### Question 1.2 **(10%)**\n",
    "It looks like the departure and arrival in `flights` were read in as floating-point numbers.  Write two functions, `extract_hour` and `extract_mins` that converts military time to hours and minutes, respectively. Hint: You may want to use modular arithmetic and integer division. Keep in mind that the data has not been cleaned and you need to check whether the extracted values are valid. Replace all the invalid values with `NaN`. The documentation for `pandas.Series.where` provided [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.where.html) should be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "kwZZitNZOp7m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10.0\n",
       "1    12.0\n",
       "2     NaN\n",
       "3     NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_hour(time):\n",
    "    \"\"\"\n",
    "    Extracts hour information from military time.\n",
    "\n",
    "    Args:\n",
    "        time (float64): series of time given in military format.\n",
    "          Takes on values in 0.0-2359.0 due to float64 representation.\n",
    "\n",
    "    Returns:\n",
    "        array (float64): series of input dimension with hour information.\n",
    "          Should only take on integer values in 0-23\n",
    "    \"\"\"\n",
    "    # [YOUR CODE HERE]\n",
    "    \n",
    "    #cleans data\n",
    "    time = time.where((time <=2359.0) & (time >= 0) , other=np.nan)\n",
    "    \n",
    "    \n",
    "    # performs integer division isolating the time\n",
    "    hour = time // 100\n",
    "\n",
    "    return hour\n",
    "\n",
    "# test your code to receive credit\n",
    "test_ser = pd.Series([1030.0, 1259.0, np.nan, 2475], dtype='float64')\n",
    "extract_hour(test_ser)\n",
    "\n",
    "# 0    10.0\n",
    "# 1    12.0\n",
    "# 2     NaN\n",
    "# 3     NaN\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "d4GK8ZwwOp7m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30.0\n",
       "1    59.0\n",
       "2     NaN\n",
       "3     NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_mins(time):\n",
    "    \"\"\"\n",
    "    Extracts minute information from military time\n",
    "\n",
    "    Args:\n",
    "        time (float64): series of time given in military format.\n",
    "          Takes on values in 0.0-2359.0 due to float64 representation.\n",
    "\n",
    "    Returns:\n",
    "        array (float64): series of input dimension with minute information.\n",
    "          Should only take on integer values in 0-59\n",
    "    \"\"\"\n",
    "    # [YOUR CODE HERE]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #cleans data\n",
    "    time = time.where((time <=2359.0) & (time >= 0) , other=np.nan)\n",
    "    \n",
    "    #EXTRACT THE MINUTES \n",
    "    mins = time % 100\n",
    "    return mins\n",
    "\n",
    "\n",
    "# test your code to receive credit\n",
    "test_ser = pd.Series([1030.0, 1259.0, np.nan, 2475], dtype='float64')\n",
    "extract_mins(test_ser)\n",
    "\n",
    "# 0    30.0\n",
    "# 1    59.0\n",
    "# 2     NaN\n",
    "# 3     NaN\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1_f8accOp7m"
   },
   "source": [
    "#### Question 1.3 **(15%)**\n",
    "\n",
    "Using your two functions above, filter the `flights` data for flights that departed 20 or more minutes later than scheduled by comparing `sched_dep_time` and `actual_dep_time`.  You need not worry about flights that were delayed to the next day for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "euijcQxCOp7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    783.0\n",
       "1    720.0\n",
       "2      NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_minofday(time):\n",
    "    \"\"\"\n",
    "    Converts military time to minute of day\n",
    "\n",
    "    Args:\n",
    "        time (float64): series of time given in military format.\n",
    "          Takes on values in 0.0-2359.0 due to float64 representation.\n",
    "\n",
    "    Returns:\n",
    "        array (float64): series of input dimension with minute of day\n",
    "\n",
    "    Example: 1:03pm is converted to 783.0\n",
    "    \"\"\"\n",
    "    \n",
    "#     extract_hour()\n",
    "#     extract_mins()\n",
    "    \n",
    "    # [YOUR CODE HERE]\n",
    "    # hours * 60 + minutes \n",
    "    hours = extract_hour(time)\n",
    "    minutes = extract_mins(time)\n",
    "    \n",
    "    minuteDay = (hours * 60 ) + minutes\n",
    "    \n",
    "    return minuteDay\n",
    "\n",
    "# Test your code  to receive credit\n",
    "ser = pd.Series([1303, 1200, 2400], dtype='float64')\n",
    "convert_to_minofday(ser)\n",
    "\n",
    "# 0    783.0\n",
    "# 1    720.0\n",
    "# 2      NaN\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "X6D-k6VCOp7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    5.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_time_diff(x, y):\n",
    "    \"\"\"\n",
    "    Calculates delay times y - x\n",
    "\n",
    "    Args:\n",
    "        x (float64): series of scheduled time given in military format.\n",
    "          Takes on values in 0.0-2359.0 due to float64 representation.\n",
    "        y (float64): series of same dimensions giving actual time\n",
    "\n",
    "    Returns:\n",
    "        array (float64): series of input dimension with delay time\n",
    "    \"\"\"\n",
    "    \n",
    "    # [YOUR CODE HERE]\n",
    "    return convert_to_minofday(y) - convert_to_minofday(x)\n",
    "#     \n",
    "\n",
    "#Test your code  to receive credit\n",
    "sched = pd.Series([1303, 1210], dtype='float64')\n",
    "actual = pd.Series([1304, 1215], dtype='float64')\n",
    "calc_time_diff(sched, actual)\n",
    "\n",
    "# 0    1.0\n",
    "# 1    5.0\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "CsBHW67EOp7n",
    "outputId": "0e0f926f-d068-4b13-d621-c3b1e49192e1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>carrier</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>flight</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>actual_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>actual_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3FLAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>630.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3APAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>600.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>AA</td>\n",
       "      <td>N3DNAA</td>\n",
       "      <td>208</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MIA</td>\n",
       "      <td>630.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1431.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day        date carrier tailnum  flight origin destination  \\\n",
       "0  2016      1    1  2016-01-01      AA  N3FLAA     208    SFO         MIA   \n",
       "1  2016      1    2  2016-01-02      AA  N3APAA     208    SFO         MIA   \n",
       "2  2016      1    3  2016-01-03      AA  N3DNAA     208    SFO         MIA   \n",
       "\n",
       "   sched_dep_time  actual_dep_time  sched_arr_time  actual_arr_time  \n",
       "0           630.0            628.0          1458.0           1431.0  \n",
       "1           600.0            553.0          1428.0           1401.0  \n",
       "2           630.0            626.0          1458.0           1431.0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "2nHqly7MOp7n",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.0\n",
       "1        7.0\n",
       "2        4.0\n",
       "3        4.0\n",
       "4        8.0\n",
       "        ... \n",
       "16848    2.0\n",
       "16849    8.0\n",
       "16850    5.0\n",
       "16851    6.0\n",
       "16852    7.0\n",
       "Length: 8795, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Apply your functions here to calculate delay between `sched_dep_time` and `actual_dep_time` on flights, to receive credit.\n",
    "# [YOUR CODE HERE]\n",
    "\n",
    "\n",
    "#ask the TAs if you need to worry about negative values\n",
    "\n",
    "\n",
    "sched_dep_time = flights['sched_dep_time']\n",
    "actual_dep_time = flights['actual_dep_time']\n",
    "\n",
    "# SHOWS ALL DELAY TIMES GREATER THAN 0\n",
    "delay_V2 = calc_time_diff(actual_dep_time,sched_dep_time) \n",
    "delay = delay_V2[delay_V2 > 0] \n",
    "\n",
    "# shows just the time \n",
    "# delay =  # Series object showing delay time\n",
    "delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151      1291.0\n",
       "153      1224.0\n",
       "154      1347.0\n",
       "158       658.0\n",
       "159      1282.0\n",
       "          ...  \n",
       "16646    1350.0\n",
       "16663    1390.0\n",
       "16668    1212.0\n",
       "16681    1336.0\n",
       "16789    1382.0\n",
       "Length: 114, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flights delayed more than 20mins\n",
    "# Code here\n",
    "\n",
    "sched_dep_time = flights['sched_dep_time']\n",
    "actual_dep_time = flights['actual_dep_time']\n",
    "\n",
    "# SHOWS ALL DELAY TIMES GREATER THAN 0\n",
    "delay_V2 = calc_time_diff(actual_dep_time,sched_dep_time) \n",
    "delay = delay_V2[delay_V2 > 20]  \n",
    "\n",
    "delayed20 = delay\n",
    "delayed20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpDFyr-0Op7o"
   },
   "source": [
    "#### Question 1.4 **(5%)**\n",
    "\n",
    "Using your answer from question 1.2, find the full name of every destination city with a flight from SFO or OAK that was delayed by 60 or more minutes.  The airport codes used in `flights` are IATA codes.  Sort the cities alphabetically. Make sure you remove duplicates. (You may find `drop_duplicates` and `sort_values` helpful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cZov-unnOp7o",
    "outputId": "6a38c0e9-452a-40ff-c14f-531b84440639"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arcata CA',\n",
       " 'Atlanta',\n",
       " 'Baltimore',\n",
       " 'Boston',\n",
       " 'Chicago',\n",
       " 'Cincinnati',\n",
       " 'Detroit',\n",
       " 'Eugene',\n",
       " 'Fort Lauderdale',\n",
       " 'Fresno',\n",
       " 'Houston',\n",
       " 'Las Vegas',\n",
       " 'Long Beach',\n",
       " 'Los Angeles',\n",
       " 'Medford',\n",
       " 'Miami',\n",
       " 'New Orleans',\n",
       " 'New York',\n",
       " 'Newark',\n",
       " 'Ontario',\n",
       " 'Palm Springs',\n",
       " 'Portland',\n",
       " 'Raleigh-durham',\n",
       " 'Redding',\n",
       " 'Redmond-Bend',\n",
       " 'Sacramento',\n",
       " 'Salt Lake City',\n",
       " 'San Diego',\n",
       " 'Santa Barbara',\n",
       " 'Seattle',\n",
       " 'Tucson',\n",
       " 'Washington']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete code here to receive credit.\n",
    "# HINT: You will need to use `delayed` and `airport` dataframes\n",
    "# [YOUR CODE HERE]\n",
    "\n",
    "\n",
    "\n",
    "# Filter flights departing from SFO or OAK\n",
    "sfo_oak_flights = flights[(flights['origin'] == 'SFO') | (flights['origin'] == 'OAK')]\n",
    "\n",
    "# Calculate the delay for each flight using calc_time_diff\n",
    "sfo_oak_flights['delay'] = calc_time_diff(sfo_oak_flights['actual_dep_time'], sfo_oak_flights['sched_dep_time'])\n",
    "\n",
    "# Filter flights with a delay of 60 or more minutes\n",
    "delayed_flights = sfo_oak_flights[sfo_oak_flights['delay'] >= 60]\n",
    "\n",
    "# Merge with the 'airports' DataFrame to get city names\n",
    "delayed_with_city = pd.merge(delayed_flights, airports, left_on='destination', right_on='iata', how='inner')\n",
    "\n",
    "# Merge with the 'airplanes' DataFrame to get airplane information\n",
    "delayed_with_airplane = pd.merge(delayed_with_city, airports, left_on='origin', right_on='iata', how='inner')\n",
    "\n",
    "# Extract the unique destination cities\n",
    "unique_destination_cities = delayed_with_city['city'].unique()\n",
    "\n",
    "# Sort the cities alphabetically\n",
    "delayed_destinations = sorted(unique_destination_cities)\n",
    "\n",
    "\n",
    "# delayed_destinations_iata = 'blabla'\n",
    "# delayed_airports = 'blabla' # Dataframe showing airports that satisfy above conditions\n",
    "# delayed_destinations = 'blabla' # Unique and sorted destination cities\n",
    "delayed_destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1sqSrolOp7o"
   },
   "source": [
    "## Part 2: Web scraping and data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AXyQmTWOp7o"
   },
   "source": [
    "### Note and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0hnPgsrOp7o"
   },
   "source": [
    "Here, you will practice collecting and processing data in Python. By the end of this exercise hopefully you should look at the wonderful world wide web without fear, comforted by the fact that anything you can see with your human eyes, a computer can see with its computer eyes. In particular, we aim to give you some familiarity with:\n",
    "\n",
    "* Using HTTP to fetch the content of a website\n",
    "* HTTP Requests (and lifecycle)\n",
    "* RESTful APIs\n",
    "    * Authentication (OAuth)\n",
    "    * Pagination\n",
    "    * Rate limiting\n",
    "* JSON vs. HTML (and how to parse each)\n",
    "* HTML traversal (CSS selectors)\n",
    "\n",
    "Since everyone loves food (presumably), the ultimate end goal of this homework will be to acquire the data to answer some questions and hypotheses about the restaurant scene in Chicago (which we will get to later). We will download __both__ the metadata on restaurants in Chicago from the Yelp API and with this metadata, retrieve the comments/reviews and ratings from users on restaurants.\n",
    "\n",
    "**Library Documentation:**\n",
    "\n",
    "For solving this part, you need to look up online documentation for the Python packages you will use:\n",
    "\n",
    "* Standard Library:\n",
    "    * [io](https://docs.python.org/3/library/io.html)\n",
    "    * [time](https://docs.python.org/3/library/time.html)\n",
    "    * [json](https://docs.python.org/3/library/json.html)\n",
    "\n",
    "* Third Party\n",
    "    * [requests](https://requests.readthedocs.io/en/latest/)\n",
    "    * [Beautiful Soup (version 4)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "    * [yelp-fusion](https://www.yelp.com/developers/documentation/v3/get_started)\n",
    "\n",
    "**Note:** You may come across a `yelp-python` library online. The library is deprecated and incompatible with the current Yelp API, so do not use the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdWZ6rndOp7p"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZkZ0Wo2Op7p"
   },
   "source": [
    "First, import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "R3_wo4lHOp7p"
   },
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_kWtJSsOp7p"
   },
   "source": [
    "### Authentication and working with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yhn4fkJOp7p"
   },
   "source": [
    "\n",
    "\n",
    "There are various authentication schemes that APIs use, listed here in relative order of complexity:\n",
    "\n",
    "* No authentication\n",
    "* [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication)\n",
    "* Cookie based user login\n",
    "* OAuth (v1.0 & v2.0, see this [post](http://stackoverflow.com/questions/4113934/how-is-oauth-2-different-from-oauth-1) explaining the differences)\n",
    "* API keys\n",
    "* Custom Authentication\n",
    "\n",
    "For the NYT example below (**Q2.1**), since it is a publicly visible page we did not need to authenticate. HTTP basic authentication isn't too common for consumer sites/applications that have the concept of user accounts (like Facebook, LinkedIn, Twitter, etc.) but is simple to setup quickly and you often encounter it on with individual password protected pages/sites.\n",
    "\n",
    "Cookie based user login is what the majority of services use when you login with a browser (i.e. username and password). Once you sign in to a service like Facebook, the response stores a cookie in your browser to remember that you have logged in (HTTP is stateless). Each subsequent request to the same domain (i.e. any page on `facebook.com`) also sends the cookie that contains the authentication information to remind Facebook's servers that you have already logged in.\n",
    "\n",
    "Many REST APIs however use OAuth (authentication using tokens) which can be thought of a programmatic way to \"login\" _another_ user. Using tokens, a user (or application) only needs to send the login credentials once in the initial authentication and as a response from the server gets a special signed token. This signed token is then sent in future requests to the server (in place of the user credentials).\n",
    "\n",
    "A similar concept common used by many APIs is to assign API Keys to each client that needs access to server resources. The client must then pass the API Key along with _every_ request it makes to the API to authenticate. This is because the server is typically relatively stateless and does not maintain a session between subsequent calls from the same client. Most APIs (including Yelp) allow you to pass the API Key via a special HTTP Header: `Authorization: Bearer <API_KEY>`. Check out the [docs](https://www.yelp.com/developers/documentation/v3/authentication) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-PeNOVnOp7q"
   },
   "source": [
    "### Question 2.1: Basic HTTP Requests w/o authentication **(5%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5oDXLwJOp7q"
   },
   "source": [
    "First, let's do the \"hello world\" of making web requests with Python to get a sense for how to programmatically access web pages: an (unauthenticated) HTTP GET to download a web page.\n",
    "\n",
    "Fill in the funtion to use `requests` to download and return the raw HTML content of the URL passed in as an argument. As an example try the following NYT article (on Youtube's algorithmic recommendation): [https://www.nytimes.com/2019/03/29/technology/youtube-online-extremism.html](https://www.nytimes.com/2019/03/29/technology/youtube-online-extremism.html)\n",
    "\n",
    "Your function should return a tuple of: (`<status_code>`, `<text>`). (Hint: look at the **Library documentation** listed earlier to see how `requests` should work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "ew3yuQz9Op7q"
   },
   "outputs": [],
   "source": [
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string):\n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "\n",
    "    # [YOUR CODE HERE]\n",
    "    r = requests.get(url)\n",
    "    status_code = r.status_code\n",
    "    text = r.text\n",
    "\n",
    "    return status_code, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "sGsT4PEHOp7q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, '<!DOCTYPE html>\\n<html >\\n<head><meta charset=\"utf-8\">\\n<title>Scraping News and Articles From Public APIs with Python | Martin Heinz | Personal Website & Blog</title>\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n<meta name=\"twitter:title\" content=\"Scraping News and Articles From Public APIs with Python\">\\n<meta name=\"twitter:text:title\" content=\"Scraping News and Articles From Public APIs with Python\">\\n<meta name=\"og:url\" content=\"https://martinheinz.dev/blog/31\">\\n<meta name=\"og:type\" content=\"article\">\\n<meta name=\"article:published_time\" content=\"2020-08-20T17:30:00Z\">\\n<meta name=\"article:section\" content=\"Technology\">\\n<meta name=\"twitter:description\" content=\"<p>\\nWhether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles c...\">\\n<meta name=\"og:description\" content=\"<p>\\nWhether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles c...\">\\n<meta name=\"description\" content=\"<p>\\nWhether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles c...\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/entry.bb8b06e9.js\"><link rel=\"preload\" as=\"style\" href=\"/_nuxt/entry.ef83fc68.css\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/_id_.5b43800d.js\"><link rel=\"preload\" as=\"style\" href=\"/_nuxt/_id_.30fd3e8c.css\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/fetch.1f75902e.js\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/asyncData.afb7569f.js\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/BaseTag.30f00e0c.js\"><link rel=\"preload\" as=\"style\" href=\"/_nuxt/BaseTag.c34349fd.css\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/SubscribeMailingList.82578239.js\"><link rel=\"preload\" as=\"style\" href=\"/_nuxt/SubscribeMailingList.e9164cc9.css\"><link rel=\"modulepreload\" as=\"script\" crossorigin href=\"/_nuxt/BaseFooter.8253640a.js\"><link rel=\"preload\" as=\"style\" href=\"/_nuxt/BaseFooter.21077193.css\"><link rel=\"prefetch\" as=\"image\" type=\"image/svg+xml\" href=\"/_nuxt/fontello.673efcac.svg\"><link rel=\"prefetch\" as=\"script\" crossorigin href=\"/_nuxt/error-component.a3cc18ca.js\"><link rel=\"stylesheet\" href=\"/_nuxt/entry.ef83fc68.css\"><link rel=\"stylesheet\" href=\"/_nuxt/_id_.30fd3e8c.css\"><link rel=\"stylesheet\" href=\"/_nuxt/BaseTag.c34349fd.css\"><link rel=\"stylesheet\" href=\"/_nuxt/SubscribeMailingList.e9164cc9.css\"><link rel=\"stylesheet\" href=\"/_nuxt/BaseFooter.21077193.css\"><style>.content{box-sizing:border-box;display:flex;flex-direction:column;min-height:100%;overflow-wrap:break-word;padding-bottom:2rem;padding-top:2rem;position:relative}.mailing-list{margin-bottom:2rem;margin-top:-3rem;text-align:center}.form-field{margin:0 auto;text-align:center;width:80%}</style><style>.nuxt-progress[data-v-1c64d9bb]{background-color:#2bbc8a;height:2px;left:0;opacity:1;position:fixed;right:0;top:0;transition:width .1s,opacity .4s;width:0;z-index:999999}.nuxt-progress.nuxt-progress-notransition[data-v-1c64d9bb]{transition:none}.nuxt-progress-failed[data-v-1c64d9bb]{background-color:#2bbc8a}</style><style>#header-post #menu-icon-tablet:hover,#header-post #menu-icon:hover{color:#2bbc8a}#header-post #top-icon-tablet{bottom:2rem;margin-left:15px;margin-right:2rem;position:fixed;right:2rem}#header-post #top-icon-tablet:hover,#header-post .active{color:#2bbc8a}#menu-icon,#menu-icon-tablet,#top-icon-tablet{margin-top:5px}.menu-inactive{color:#c9cacc}.menu-active{color:#2bbc8a}#toc{margin-top:1rem;max-height:calc(95vh - 7rem);max-width:20em;overflow:auto;padding-right:2rem;text-align:right}#header-post #nav{color:#2bbc8a;font-size:.82rem;font-style:normal;font-weight:200;letter-spacing:.01em}#header-post #nav ul a{color:#2bbc8a;margin-right:18px}#header-post #nav ul a:hover{background-image:linear-gradient(transparent,transparent 5px,#2bbc8a 0,#2bbc8a);background-position:bottom;background-repeat:repeat-x;background-size:100% 6px}</style><style>#actions{margin-right:2rem;margin-top:2rem;position:absolute;right:0;width:auto}#actions .info{display:inline;float:right;font-style:italic;overflow:hidden;white-space:nowrap}</style><style>.article-tag .tag-link{background-image:linear-gradient(transparent,transparent 10px,#d480aa 0,#d480aa);background-position:bottom;background-repeat:repeat-x;background-size:100% 6px}.tag-link a:hover{box-shadow:inset 0 0 0 #fff,inset 0 -2px 0 #d480aa}.tag-link a{background-image:linear-gradient(transparent,transparent 5px,#c9cacc 0,#c9cacc);background-position:bottom;background-repeat:repeat-x;background-size:100% 6px;color:#c9cacc;text-decoration:none}article header .posttitle{margin-bottom:0;margin-top:0}.posttitle,article header .posttitle{font-size:1.5em;line-height:1.25;text-transform:none}header .meta{margin-bottom:1rem;margin-top:0}header .meta *{color:#ccc;font-size:.85rem}header .author{font-weight:700;letter-spacing:.01em;margin:0 5px;text-transform:uppercase}header .postdate{display:inline}@media (min-width:480px){.article-tag{display:inline}.article-tag:before{content:\" | \"}}span:first-child{margin-left:0}</style><style>code[class*=language-],pre[class*=language-]{background:#282c34;color:#abb2bf;direction:ltr;font-family:Fira Code,Fira Mono,Menlo,Consolas,DejaVu Sans Mono,monospace;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;text-align:left;text-shadow:0 1px rgba(0,0,0,.3);white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection{background:#3e4451;color:inherit;text-shadow:none}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection{background:#3e4451;color:inherit;text-shadow:none}pre[class*=language-]{border-radius:.3em;margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.2em .3em;white-space:normal}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}.token.cdata,.token.comment,.token.prolog{color:#5c6370}.token.doctype,.token.entity,.token.punctuation{color:#abb2bf}.token.atrule,.token.attr-name,.token.boolean,.token.class-name,.token.constant,.token.number{color:#d19a66}.token.keyword{color:#c678dd}.token.deleted,.token.important,.token.property,.token.symbol,.token.tag{color:#e06c75}.token.attr-value,.token.attr-value>.token.punctuation,.token.builtin,.token.char,.token.inserted,.token.regex,.token.selector,.token.string{color:#98c379}.token.function,.token.operator,.token.variable{color:#61afef}.token.url{color:#56b6c2}.token.attr-value>.token.punctuation.attr-equals,.token.special-attr>.token.attr-value>.token.value.css{color:#abb2bf}.language-css .token.selector{color:#e06c75}.language-css .token.property{color:#abb2bf}.language-css .token.function,.language-css .token.url>.token.function{color:#56b6c2}.language-css .token.url>.token.string.url{color:#98c379}.language-css .token.atrule .token.rule,.language-css .token.important,.language-javascript .token.operator{color:#c678dd}.language-javascript .token.template-string>.token.interpolation>.token.interpolation-punctuation.punctuation{color:#be5046}.language-json .token.operator{color:#abb2bf}.language-json .token.null.keyword{color:#d19a66}.language-markdown .token.url,.language-markdown .token.url-reference.url>.token.string,.language-markdown .token.url>.token.operator{color:#abb2bf}.language-markdown .token.url>.token.content{color:#61afef}.language-markdown .token.url-reference.url,.language-markdown .token.url>.token.url{color:#56b6c2}.language-markdown .token.blockquote.punctuation,.language-markdown .token.hr.punctuation{color:#5c6370;font-style:italic}.language-markdown .token.code-snippet{color:#98c379}.language-markdown .token.bold .token.content{color:#d19a66}.language-markdown .token.italic .token.content{color:#c678dd}.language-markdown .token.list.punctuation,.language-markdown .token.strike .token.content,.language-markdown .token.strike .token.punctuation,.language-markdown .token.title.important>.token.punctuation{color:#e06c75}.token.bold{font-weight:700}.token.comment,.token.italic{font-style:italic}.token.entity{cursor:help}.token.namespace{opacity:.8}.token.token.cr:before,.token.token.lf:before,.token.token.space:before,.token.token.tab:not(:empty):before{color:rgba(171,178,191,.15);text-shadow:none}div.code-toolbar>.toolbar.toolbar>.toolbar-item{margin-right:.4em}div.code-toolbar>.toolbar.toolbar>.toolbar-item>a,div.code-toolbar>.toolbar.toolbar>.toolbar-item>button,div.code-toolbar>.toolbar.toolbar>.toolbar-item>span{background:#3a3f4b;border-radius:.3em;color:#828997;padding:.1em .4em}div.code-toolbar>.toolbar.toolbar>.toolbar-item>a:focus,div.code-toolbar>.toolbar.toolbar>.toolbar-item>a:hover,div.code-toolbar>.toolbar.toolbar>.toolbar-item>button:focus,div.code-toolbar>.toolbar.toolbar>.toolbar-item>button:hover,div.code-toolbar>.toolbar.toolbar>.toolbar-item>span:focus,div.code-toolbar>.toolbar.toolbar>.toolbar-item>span:hover{background:#3e4451;color:#abb2bf}.line-highlight.line-highlight{background:rgba(153,187,255,.04)}.line-highlight.line-highlight:before,.line-highlight.line-highlight[data-end]:after{background:#3a3f4b;border-radius:.3em;box-shadow:0 2px 0 0 rgba(0,0,0,.2);color:#abb2bf;padding:.1em .6em}pre[id].linkable-line-numbers.linkable-line-numbers span.line-numbers-rows>span:hover:before{background-color:rgba(153,187,255,.04)}.command-line .command-line-prompt,.line-numbers.line-numbers .line-numbers-rows{border-right-color:rgba(171,178,191,.15)}.command-line .command-line-prompt>span:before,.line-numbers .line-numbers-rows>span:before{color:#636d83}.rainbow-braces .token.token.punctuation.brace-level-1,.rainbow-braces .token.token.punctuation.brace-level-5,.rainbow-braces .token.token.punctuation.brace-level-9{color:#e06c75}.rainbow-braces .token.token.punctuation.brace-level-10,.rainbow-braces .token.token.punctuation.brace-level-2,.rainbow-braces .token.token.punctuation.brace-level-6{color:#98c379}.rainbow-braces .token.token.punctuation.brace-level-11,.rainbow-braces .token.token.punctuation.brace-level-3,.rainbow-braces .token.token.punctuation.brace-level-7{color:#61afef}.rainbow-braces .token.token.punctuation.brace-level-12,.rainbow-braces .token.token.punctuation.brace-level-4,.rainbow-braces .token.token.punctuation.brace-level-8{color:#c678dd}pre.diff-highlight>code .token.token.deleted:not(.prefix),pre>code.diff-highlight .token.token.deleted:not(.prefix){background-color:rgba(255,82,102,.15)}pre.diff-highlight>code .token.token.deleted:not(.prefix) ::-moz-selection,pre.diff-highlight>code .token.token.deleted:not(.prefix)::-moz-selection,pre>code.diff-highlight .token.token.deleted:not(.prefix) ::-moz-selection,pre>code.diff-highlight .token.token.deleted:not(.prefix)::-moz-selection{background-color:rgba(251,86,105,.25)}pre.diff-highlight>code .token.token.deleted:not(.prefix) ::selection,pre.diff-highlight>code .token.token.deleted:not(.prefix)::selection,pre>code.diff-highlight .token.token.deleted:not(.prefix) ::selection,pre>code.diff-highlight .token.token.deleted:not(.prefix)::selection{background-color:rgba(251,86,105,.25)}pre.diff-highlight>code .token.token.inserted:not(.prefix),pre>code.diff-highlight .token.token.inserted:not(.prefix){background-color:rgba(25,255,91,.15)}pre.diff-highlight>code .token.token.inserted:not(.prefix) ::-moz-selection,pre.diff-highlight>code .token.token.inserted:not(.prefix)::-moz-selection,pre>code.diff-highlight .token.token.inserted:not(.prefix) ::-moz-selection,pre>code.diff-highlight .token.token.inserted:not(.prefix)::-moz-selection{background-color:rgba(56,224,98,.25)}pre.diff-highlight>code .token.token.inserted:not(.prefix) ::selection,pre.diff-highlight>code .token.token.inserted:not(.prefix)::selection,pre>code.diff-highlight .token.token.inserted:not(.prefix) ::selection,pre>code.diff-highlight .token.token.inserted:not(.prefix)::selection{background-color:rgba(56,224,98,.25)}.prism-previewer-gradient.prism-previewer-gradient div,.prism-previewer.prism-previewer:before{border-color:#262931}.prism-previewer-color.prism-previewer-color:before,.prism-previewer-easing.prism-previewer-easing:before,.prism-previewer-gradient.prism-previewer-gradient div{border-radius:.3em}.prism-previewer.prism-previewer:after{border-top-color:#262931}.prism-previewer-flipped.prism-previewer-flipped.after{border-bottom-color:#262931}.prism-previewer-angle.prism-previewer-angle:before,.prism-previewer-easing.prism-previewer-easing,.prism-previewer-time.prism-previewer-time:before{background:#31363f}.prism-previewer-angle.prism-previewer-angle circle,.prism-previewer-time.prism-previewer-time circle{stroke:#abb2bf;stroke-opacity:1}.prism-previewer-easing.prism-previewer-easing circle,.prism-previewer-easing.prism-previewer-easing line,.prism-previewer-easing.prism-previewer-easing path{stroke:#abb2bf}.prism-previewer-easing.prism-previewer-easing circle{fill:transparent}p{margin:7px 0}ol,ul{padding:0 0 0 40px}li{margin-bottom:3px}div.content{padding-top:32px}img{padding-bottom:20px;padding-top:20px}pre{border:1px dotted #666;margin-bottom:10px!important;margin-top:10px!important}</style><style>*[data-v-a3042193]{box-sizing:border-box}.navbar[data-v-a3042193]{background:#212326;border-top:1px solid #666;bottom:0;height:60px;left:0;margin-bottom:0;margin-top:3px;overflow:auto;position:fixed;right:0;text-align:center;transform:translateZ(0);transition:all .1s ease-out;white-space:nowrap;width:100%}.navbar.navbar--hidden[data-v-a3042193]{transform:translate3d(0,100%,0)}.footer-action[data-v-a3042193]{color:#2bbc8a;display:inline-block;padding-left:.7rem;padding-right:.7rem;padding-top:1.1rem}.footer-action-text[data-v-a3042193],.footer-action-text-short[data-v-a3042193]{bottom:0;left:5px;position:relative}.content a[data-v-a3042193]{background-image:linear-gradient(transparent,transparent,transparent,transparent);color:#2bbc8a!important;text-decoration:none!important}.content a[data-v-a3042193]:hover{background-image:linear-gradient(transparent,transparent)}@media screen and (min-width:601px){.navbar[data-v-a3042193]{display:none}}@media screen and (min-width:501px) and (max-width:601px){.footer-action-text-short[data-v-a3042193]{display:none}.footer-action-text[data-v-a3042193]{display:unset}}@media screen and (max-width:500px){.footer-action-text-short[data-v-a3042193]{display:unset}.footer-action-text[data-v-a3042193]{display:none}}@media screen and (max-width:400px){.footer-action-text[data-v-a3042193],.footer-action-text-short[data-v-a3042193]{display:none}}</style></head>\\n<body ><div id=\"__nuxt\"><div class=\"scroll-smooth\"><!--[--><div><div><!--[--><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!--]--><div id=\"header-post\"><i id=\"menu-icon\" href=\"#\" class=\"icon-menu\"></i><span id=\"menu\" style=\"visibility:visible;display:none;\"><span id=\"nav\"><ul><!--[--><a href=\"/\">Home</a><a href=\"/contact\">Contact</a><a href=\"/subscribe\">Subscribe</a><a href=\"https://ko-fi.com/martinheinz\">Tip Jar</a><!--]--></ul></span><br><span id=\"actions\"><a id=\"top\" href=\"#\"></a><ul><li><a href=\"/blog/30\"><i class=\"icon-left-open hoverable\" href=\"/blog/30\"></i></a></li><li><a href=\"/blog/32\"><i class=\"icon-right-open hoverable\" href=\"/blog/32\"></i></a></li><li><i class=\"icon-up-open hoverable\" id=\"top_icon\"></i></li></ul><div class=\"info\" style=\"display:none;\">Previous post</div><div class=\"info\" style=\"display:none;\">Next post</div><div class=\"info\" style=\"display:none;\">Back to top</div></span><br></span></div><div class=\"content\"><header><h1 class=\"posttitle\" itemprop=\"name headline\">Scraping News and Articles From Public APIs with Python</h1><div class=\"meta\"><span class=\"author\" itemprop=\"author\" itemscope=\"\" itemtype=\"http://schema.org/Person\"><span itemprop=\"name\">Martin</span></span><div class=\"postdate\"><time datetime=\"2020-08-20T17:30:00Z\" itemprop=\"datePublished\">Aug 20, 2020</time></div><div class=\"article-tag\"><i class=\"icon-tag\"></i><!--[--><a class=\"tag-link\" href=\"/tag/python/\">Python</a><a class=\"tag-link\" href=\"/tag/api/\">API</a><!--]--></div></div></header><div><article><div class=\"content\"><div><!--[--><p> Whether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles can be challenging though as you will have to go through quite a few hoops to get to the actual data - finding the right news sources, exploring their APIs, figuring out how to authenticate against them and finally scraping the data. That&#39;s a lot of work and no fun. </p><p> So, to save you some time and get you started, here&#39;s list of public news APIs that I was able to find, with explanation how authenticate against them, query them and most importantly examples for how to get all the data you need from them! </p><h2>NY Times</h2><p> First and the best source of data is in my opinion <i>New Your Times</i>. To start using its API you need to create an account at <a href=\"https://developer.nytimes.com/accounts/create\">https://developer.nytimes.com/accounts/create</a> and an application at <a href=\"https://developer.nytimes.com/my-apps/new-app\">https://developer.nytimes.com/my-apps/new-app</a>. When creating the application you get to choose which APIs to activate - I recommend activating at least <i>Most Popular</i>, <i>Article Search</i>, <i>Top Stories</i> and <i>Archive APIs</i>. When your application is created you will be presented with the key which you will use to interact all the selected APIs, so copy it and let&#39;s start querying! </p><p> The simplest query we can do with <i>NY Times</i> API is look up for current top stories: </p><pre><code class=\"language-python\">\\nimport requests\\nimport os\\nfrom pprint import pprint\\n\\napikey = os.getenv(&#39;NYTIMES_APIKEY&#39;, &#39;...&#39;)\\n\\n# Top Stories:\\n# https://developer.nytimes.com/docs/top-stories-product/1/overview\\nsection = &quot;science&quot;\\nquery_url = f&quot;https://api.nytimes.com/svc/topstories/v2/{section}.json?api-key={apikey}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> The snippet above is very straightforward. We run a <code class=\"inline\">GET</code> request against <code class=\"inline\">topstories/v2</code> endpoint supplying <code class=\"inline\">section</code> name and our API key. Section in this case is <i>science</i>, but <i>NY Times</i> provides a lot of other options here, e.g. fashion, health, sports or theater. Full list can be found <a href=\"https://developer.nytimes.com/docs/top-stories-product/1/overview\">here</a>. This specific request would produce response that would look something like this: </p><pre><code class=\"language-python\">\\n{ &#39;last_updated&#39;: &#39;2020-08-09T08:07:44-04:00&#39;,\\n &#39;num_results&#39;: 25,\\n &#39;results&#39;: [{&#39;abstract&#39;: &#39;New Zealand marked 100 days with no new reported &#39;\\n                          &#39;cases of local coronavirus transmission. France &#39;\\n                          &#39;will require people to wear masks in crowded &#39;\\n                          &#39;outdoor areas.&#39;,\\n              &#39;byline&#39;: &#39;&#39;,\\n              &#39;created_date&#39;: &#39;2020-08-09T08:00:12-04:00&#39;,\\n              &#39;item_type&#39;: &#39;Article&#39;,\\n              &#39;multimedia&#39;: [{&#39;caption&#39;: &#39;&#39;,\\n                              &#39;copyright&#39;: &#39;The New York Times&#39;,\\n                              &#39;format&#39;: &#39;superJumbo&#39;,\\n                              &#39;height&#39;: 1080,\\n                              &#39;subtype&#39;: &#39;photo&#39;,\\n                              &#39;type&#39;: &#39;image&#39;,\\n                              &#39;url&#39;: &#39;https://static01.nyt.com/images/2020/08/03/us/us-briefing-promo-image-print/us-briefing-promo-image-superJumbo.jpg&#39;,\\n                              &#39;width&#39;: 1920},\\n                             ],\\n              &#39;published_date&#39;: &#39;2020-08-09T08:00:12-04:00&#39;,\\n              &#39;section&#39;: &#39;world&#39;,\\n              &#39;short_url&#39;: &#39;https://nyti.ms/3gH9NXP&#39;,\\n              &#39;title&#39;: &#39;Coronavirus Live Updates: DeWine Stresses Tests &#39;\\n                       &#39;Value, Even After His False Positive&#39;,\\n              &#39;uri&#39;: &#39;nyt://article/27dd9f30-ad63-52fe-95ab-1eba3d6a553b&#39;,\\n              &#39;url&#39;: &#39;https://www.nytimes.com/2020/08/09/world/coronavirus-covid-19.html&#39;},\\n             ]\\n }\\n</code></pre><p> Next and probably the most useful endpoint when you are trying to get some specific set of data is the <i>article search</i> endpoint: </p><pre><code class=\"language-python\">\\n# Article Search:\\n# https://api.nytimes.com/svc/search/v2/articlesearch.json?q=&lt;QUERY&gt;&amp;api-key=&lt;APIKEY&gt;\\n# Use - https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get to explore API\\n\\nquery = &quot;politics&quot;\\nbegin_date = &quot;20200701&quot;  # YYYYMMDD\\nfilter_query = &quot;\\\\&quot;body:(\\\\&quot;Trump\\\\&quot;) AND glocations:(\\\\&quot;WASHINGTON\\\\&quot;)\\\\&quot;&quot;  # http://www.lucenetutorial.com/lucene-query-syntax.html\\npage = &quot;0&quot;  # &lt;0-100&gt;\\nsort = &quot;relevance&quot;  # newest, oldest\\nquery_url = f&quot;https://api.nytimes.com/svc/search/v2/articlesearch.json?&quot; \\\\\\n            f&quot;q={query}&quot; \\\\\\n            f&quot;&amp;api-key={apikey}&quot; \\\\\\n            f&quot;&amp;begin_date={begin_date}&quot; \\\\\\n            f&quot;&amp;fq={filter_query}&quot; \\\\\\n            f&quot;&amp;page={page}&quot; \\\\\\n            f&quot;&amp;sort={sort}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> This endpoint features lots of filtering options. The only mandatory field is <code class=\"inline\">q</code> (query), which is the search term. Beyond that you can mix and match filter query, date range (<code class=\"inline\">begin_date</code>, <code class=\"inline\">end_date</code>), page number, sort order and facet fields. The filter query (<code class=\"inline\">fq</code>) is interesting one, as it allows use of <i>Lucene query syntax</i>, which can be used to create complex filters with logical operators (<code class=\"inline\">AND</code>, <code class=\"inline\">OR</code>), negations or wildcards. Nice tutorial can be found <a href=\"http://www.lucenetutorial.com/lucene-query-syntax.html\">here</a>. </p><p> Example response for above query might like this (some fields were removed for clarity): </p><pre><code class=\"language-python\">\\n{&#39;response&#39;: {&#39;docs&#39;: [{&#39;_id&#39;: &#39;nyt://article/0bf06be1-6699-527f-acb0-09fdd8abb6f6&#39;,\\n                        &#39;abstract&#39;: &#39;The president sidestepped Congress when it became clear that his nominee for a &#39;\\n                                    &#39;top Defense Department position would not win Senate approval.&#39;,\\n                        &#39;byline&#39;: {&#39;original&#39;: &#39;By Helene Cooper&#39;},\\n                        &#39;document_type&#39;: &#39;article&#39;,\\n                        &#39;headline&#39;: {&#39;main&#39;: &#39;Trump Puts Pentagon in Political Crossfire With Tata Appointment&#39;,\\n                                     &#39;print_headline&#39;: &#39;Bypassing Congress to Appoint Ally, Trump Puts Pentagon in Political Crossfire&#39;},\\n                        &#39;keywords&#39;: [{&#39;major&#39;: &#39;N&#39;, &#39;name&#39;: &#39;subject&#39;, &#39;rank&#39;: 1,\\n                                      &#39;value&#39;: &#39;United States Politics and Government&#39;},\\n                                     {&#39;major&#39;: &#39;N&#39;, &#39;name&#39;: &#39;subject&#39;, &#39;rank&#39;: 2,\\n                                      &#39;value&#39;: &#39;Appointments and Executive Changes&#39;},\\n                                     {&#39;major&#39;: &#39;N&#39;, &#39;name&#39;: &#39;subject&#39;, &#39;rank&#39;: 3,\\n                                      &#39;value&#39;: &#39;Presidential Election of 2020&#39;}],\\n                        &#39;lead_paragraph&#39;: &#39;WASHINGTON  In making an end run around Congress to appoint Anthony J. Tata, a retired brigadier &#39;\\n                                          &#39;general with a history of Islamophobic and other inflammatory views, to a top Defense Department &#39;\\n                                          &#39;post, President Trump has once again put the military exactly where it does not want to be: in &#39;\\n                                          &#39;the middle of a political battle that could hurt bipartisan support for the Pentagon.&#39;,\\n                        &#39;multimedia&#39;: [],\\n                        &#39;news_desk&#39;: &#39;Washington&#39;,\\n                        &#39;pub_date&#39;: &#39;2020-08-03T21:19:00+0000&#39;,\\n                        &#39;section_name&#39;: &#39;U.S.&#39;,\\n                        &#39;source&#39;: &#39;The New York Times&#39;,\\n                        &#39;subsection_name&#39;: &#39;Politics&#39;,\\n                        &#39;type_of_material&#39;: &#39;News&#39;,\\n                        &#39;uri&#39;: &#39;nyt://article/0bf06be1-6699-527f-acb0-09fdd8abb6f6&#39;,\\n                        &#39;web_url&#39;: &#39;https://www.nytimes.com/2020/08/03/us/politics/tata-pentagon.html&#39;,\\n                        &#39;word_count&#39;: 927}]}}\\n</code></pre><p> Last endpoint for <i>NY Times</i> that I will show here is their <i>Archive API</i> which returns list of articles for given month going back all the way to 1851! This can be very useful if you need bulk data and don&#39;t really need to search for specific terms. </p><pre><code class=\"language-python\">\\n# Archive Search\\n# https://developer.nytimes.com/docs/archive-product/1/overview\\n\\nyear = &quot;1852&quot;  # &lt;1851 - 2020&gt;\\nmonth = &quot;6&quot;  # &lt;1 - 12&gt;\\nquery_url = f&quot;https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apikey}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> The query above searches for all articles from June of 1852 and from the result below we can see that even though we search for really old articles we still got 1888 hits. That said, most of these lack most of the useful data like keywords, word counts, author, etc. so you are probably better off searching for little more recent articles. </p><pre><code class=\"language-python\">\\n{&#39;response&#39;: {\\n        &#39;meta&#39;: {&#39;hits&#39;: 1888},\\n        &#39;docs&#39;: [{&#39;_id&#39;: &#39;nyt://article/fada2905-0108-54a9-8729-ae9cda8b9528&#39;,\\n                        &#39;byline&#39;: {&#39;organization&#39;: None, &#39;original&#39;: None, &#39;person&#39;: []},\\n                        &#39;document_type&#39;: &#39;article&#39;,\\n                        &#39;headline&#39;: {&#39;content_kicker&#39;: None, &#39;kicker&#39;: &#39;1&#39;,\\n                                     &#39;main&#39;: &#39;Sentence for Manslaughter.&#39;,\\n                                     &#39;name&#39;: None,\\n                                     &#39;print_headline&#39;: &#39;Sentence for Manslaughter.&#39;},\\n                        &#39;keywords&#39;: [], &#39;news_desk&#39;: &#39;None&#39;,\\n                        &#39;print_page&#39;: &#39;3&#39;,\\n                        &#39;pub_date&#39;: &#39;1852-06-29T05:00:00+0000&#39;,\\n                        &#39;section_name&#39;: &#39;Archives&#39;,\\n                        &#39;source&#39;: &#39;The New York Times&#39;,\\n                        &#39;type_of_material&#39;: &#39;Archives&#39;,\\n                        &#39;uri&#39;: &#39;nyt://article/fada2905-0108-54a9-8729-ae9cda8b9528&#39;,\\n                        &#39;web_url&#39;: &#39;https://www.nytimes.com/1852/06/29/archives/sentence-for-manslaughter.html&#39;,\\n                        &#39;word_count&#39;: 0},\\n                ...]}\\n}\\n</code></pre><p> These were just some of the (in my opinion) more useful APIs provided by <i>NY Times</i>. Beside these, there are bunch more available at <a href=\"https://developer.nytimes.com/apis\">https://developer.nytimes.com/apis</a>. To explore each API, I would also recommend playing with query builder like <a href=\"https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get\">the one for article search</a>, which lets you and build and execute your test query right on the website without any coding. </p><h2>The Guardian</h2><p> Next up is another great source of news and articles - <i>The Guardian</i>. Same as with <i>NY Times</i>, we first need to sign up for an API key. You can do so at <a href=\"https://bonobo.capi.gutools.co.uk/register/developer\">https://bonobo.capi.gutools.co.uk/register/developer</a> and you will receive your key in an email. With that out of the way, we can navigate to <a href=\"https://open-platform.theguardian.com/documentation/\">API documentation</a> and start querying the API. </p><p> Let&#39;s start simply by querying content sections of <i>The Guardian</i>: </p><pre><code class=\"language-python\">\\n# https://open-platform.theguardian.com/documentation/section\\nquery = &quot;science&quot;\\nquery_url = f&quot;https://content.guardianapis.com/sections?&quot; \\\\\\n            f&quot;api-key={apikey}&quot; \\\\\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n\\n{&#39;response&#39;: {&#39;results&#39;: [{&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/science&#39;,\\n                           &#39;editions&#39;: [{&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/science&#39;,\\n                                         &#39;code&#39;: &#39;default&#39;,\\n                                         &#39;id&#39;: &#39;science&#39;,\\n                                         &#39;webTitle&#39;: &#39;Science&#39;,\\n                                         &#39;webUrl&#39;: &#39;https://www.theguardian.com/science&#39;}],\\n                           &#39;id&#39;: &#39;science&#39;,\\n                           &#39;webTitle&#39;: &#39;Science&#39;,\\n                           &#39;webUrl&#39;: &#39;https://www.theguardian.com/science&#39;}],\\n              &#39;status&#39;: &#39;ok&#39;,\\n              &#39;total&#39;: 1,\\n              &#39;userTier&#39;: &#39;developer&#39;}}\\n</code></pre><p> These sections group content into topics, which can be useful if you are looking for specific type of content, e.g. <i>science</i> or <i>technology</i>. If we omit the query (<code class=\"inline\">q</code>) parameter, we will instead receive full list of sections, which is about 75 records. </p><p> Moving on to something little more interesting - searching by <i>tags</i>: </p><pre><code class=\"language-python\">\\n# https://open-platform.theguardian.com/documentation/tag\\nquery = &quot;weather&quot;\\nsection = &quot;news&quot;\\npage = &quot;1&quot;\\nquery_url = f&quot;http://content.guardianapis.com/tags?&quot; \\\\\\n            f&quot;api-key={apikey}&quot; \\\\\\n            f&quot;&amp;q={query}&quot; \\\\\\n            f&quot;&amp;page={page}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n\\n{&#39;response&#39;: {&#39;currentPage&#39;: 1,\\n              &#39;pageSize&#39;: 10,\\n              &#39;pages&#39;: 139,\\n              &#39;results&#39;: [\\n                          {&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/australia-news/australia-weather&#39;,\\n                           &#39;id&#39;: &#39;australia-news/australia-weather&#39;,\\n                           &#39;sectionId&#39;: &#39;australia-news&#39;,\\n                           &#39;sectionName&#39;: &#39;Australia news&#39;,\\n                           &#39;type&#39;: &#39;keyword&#39;,\\n                           &#39;webTitle&#39;: &#39;Australia weather&#39;,\\n                           &#39;webUrl&#39;: &#39;https://www.theguardian.com/australia-news/australia-weather&#39;},\\n                          {&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/world/extreme-weather&#39;,\\n                           &#39;id&#39;: &#39;world/extreme-weather&#39;,\\n                           &#39;sectionId&#39;: &#39;world&#39;,\\n                           &#39;sectionName&#39;: &#39;World news&#39;,\\n                           &#39;type&#39;: &#39;keyword&#39;,\\n                           &#39;webTitle&#39;: &#39;Extreme weather&#39;,\\n                           &#39;webUrl&#39;: &#39;https://www.theguardian.com/world/extreme-weather&#39;},\\n                          ],\\n              &#39;startIndex&#39;: 1,\\n              &#39;status&#39;: &#39;ok&#39;,\\n              &#39;total&#39;: 1385,\\n              &#39;userTier&#39;: &#39;developer&#39;}}\\n</code></pre><p> This query looks quite similar to the previous one and also returns similar kinds of data. Tags also group content into categories, but there are a lot more tags (around 50000) than sections. Each of these tags have structure like for example <code class=\"inline\">world/extreme-weather</code>. These are very useful when doing search for actual articles, which is what we will do next. </p><p> The one thing you really came here for is article search and for that we will use <a href=\"https://open-platform.theguardian.com/documentation/search\">https://open-platform.theguardian.com/documentation/search</a>: </p><pre><code class=\"language-python\">\\nquery = &quot;(hurricane OR storm)&quot;\\nquery_fields = &quot;body&quot;\\nsection = &quot;news&quot;  # https://open-platform.theguardian.com/documentation/section\\ntag = &quot;world/extreme-weather&quot;  # https://open-platform.theguardian.com/documentation/tag\\nfrom_date = &quot;2019-01-01&quot;\\nquery_url = f&quot;https://content.guardianapis.com/search?&quot; \\\\\\n            f&quot;api-key={apikey}&quot; \\\\\\n            f&quot;&amp;q={query}&quot; \\\\\\n            f&quot;&amp;query-fields={query_fields}&quot; \\\\\\n            f&quot;ion={section}&quot; \\\\\\n            f&quot;&amp;tag={tag}&quot; \\\\\\n            f&quot;&amp;from-date={from_date}&quot; \\\\\\n            f&quot;&amp;show-fields=headline,byline,starRating,shortUrl&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> The reason I first showed you <i>section</i> and <i>tag</i> search is that those can be used in the article search. Above you can see that we used <code class=\"inline\">section</code> and <code class=\"inline\">tag</code> parameters to narrow down our search, which values can be found using previously shown queries. Apart from these parameters, we also included the obvious <code class=\"inline\">q</code> parameter for our search query, but also starting date using <code class=\"inline\">from-date</code> as well as <code class=\"inline\">show-fields</code> parameter, which allows us to request extra fields related to the content - in this case those would be headline, byline, rating and shortened URL. There&#39;s bunch more of those with full list available <a href=\"https://open-platform.theguardian.com/documentation/search\">here</a>. </p><p> And as with all the previous ones, here is example response: </p><pre><code class=\"language-python\">\\n{&#39;response&#39;: {&#39;currentPage&#39;: 1, &#39;orderBy&#39;: &#39;relevance&#39;, &#39;pageSize&#39;: 10, &#39;pages&#39;: 1,\\n              &#39;results&#39;: [{&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/news/2019/dec/19/weatherwatch-storms-hit-france-and-iceland-as-australia-overheats&#39;,\\n                           &#39;fields&#39;: {&#39;byline&#39;: &#39;Daniel Gardner (MetDesk)&#39;,\\n                                      &#39;headline&#39;: &#39;Weatherwatch: storms hit France and Iceland as Australia overheats&#39;,\\n                                      &#39;shortUrl&#39;: &#39;https://gu.com/p/dv4dq&#39;},\\n                           &#39;id&#39;: &#39;news/2019/dec/19/weatherwatch-storms-hit-france-and-iceland-as-australia-overheats&#39;,\\n                           &#39;pillarId&#39;: &#39;pillar/news&#39;,\\n                           &#39;sectionId&#39;: &#39;news&#39;,\\n                           &#39;type&#39;: &#39;article&#39;,\\n                           &#39;webPublicationDate&#39;: &#39;2019-12-19T11:33:52Z&#39;,\\n                           &#39;webTitle&#39;: &#39;Weatherwatch: storms hit France and &#39;\\n                                       &#39;Iceland as Australia overheats&#39;,\\n                           &#39;webUrl&#39;: &#39;https://www.theguardian.com/news/2019/dec/19/weatherwatch-storms-hit-france-and-iceland-as-australia-overheats&#39;},\\n                          {&#39;apiUrl&#39;: &#39;https://content.guardianapis.com/news/2020/jan/31/weatherwatch-how-repeated-flooding-can-shift-levees&#39;,\\n                           &#39;fields&#39;: {&#39;byline&#39;: &#39;David Hambling&#39;,\\n                                      &#39;headline&#39;: &#39;Weatherwatch: how repeated &#39;\\n                                                  &#39;flooding can shift levees&#39;,\\n                                      &#39;shortUrl&#39;: &#39;https://gu.com/p/d755m&#39;},\\n                           &#39;id&#39;: &#39;news/2020/jan/31/weatherwatch-how-repeated-flooding-can-shift-levees&#39;,\\n                           &#39;pillarId&#39;: &#39;pillar/news&#39;,\\n                           &#39;sectionId&#39;: &#39;news&#39;,\\n                           &#39;type&#39;: &#39;article&#39;,\\n                           &#39;webPublicationDate&#39;: &#39;2020-01-31T21:30:00Z&#39;,\\n                           &#39;webTitle&#39;: &#39;Weatherwatch: how repeated flooding can shift levees&#39;,\\n                           &#39;webUrl&#39;: &#39;https://www.theguardian.com/news/2020/jan/31/weatherwatch-how-repeated-flooding-can-shift-levees&#39;}],\\n              &#39;startIndex&#39;: 1, &#39;status&#39;: &#39;ok&#39;, &#39;total&#39;: 7, &#39;userTier&#39;: &#39;developer&#39;}}\\n</code></pre><h2>HackerNews</h2><p> For more tech oriented source of news, one might turn to <i>HackerNews</i>, which also has its public REST API. It&#39;s documented on <a href=\"https://github.com/HackerNews/API\">https://github.com/HackerNews/API</a>. This API, as you will see, is in version <code class=\"inline\">v0</code> and is currently very bare-bones, meaning it doesn&#39;t really provide specific endpoints to - for example - query articles, comments or users. </p><p> But even though it&#39;s very basic it still provides all that&#39;s necessary to, for example, get top stories: </p><pre><code class=\"language-python\">\\nquery_type = &quot;top&quot;  # top/best/new, also ask/show/job\\nquery_url = f&quot;https://hacker-news.firebaseio.com/v0/{query_type}stories.json?print=pretty&quot;  # Top Stories\\nr = requests.get(query_url)\\nids = r.json()\\n\\ntop = ids[:10]\\nfor story in top:\\n    query_url = f&quot;https://hacker-news.firebaseio.com/v0/item/{story}.json?print=pretty&quot;\\n    r = requests.get(query_url)\\n    pprint(r.json())\\n</code></pre><p> The snippet above is not nearly as obvious as the previous ones, so let&#39;s look at it more closely. We first send request to API endpoint (<code class=\"inline\">v0/topstories</code>), which doesn&#39;t return top stories as you would expect, but really just their IDs. To get the actual stories we take these IDs (first 10 of them) and send requests to <code class=\"inline\">v0/item/&lt;ID&gt;</code> endpoint which returns data for each of these individual items, which in this case happens to be a story. </p><p> You surely noticed that the query URL was parametrized with <code class=\"inline\">query_type</code>. That&#39;s because, HackerNews API also has similar endpoints for all the top sections of the website, that being - ask, show, job or new. </p><p> One nice thing about this API is that it doesn&#39;t require authenticate, so you don&#39;t need to request API key and don&#39;t need to worry about rate limiting like with the other ones. </p><p> Running this code would land response that looks something like this: </p><pre><code class=\"language-python\">\\n{&#39;by&#39;: &#39;rkwz&#39;,\\n &#39;descendants&#39;: 217,\\n &#39;id&#39;: 24120311,\\n &#39;kids&#39;: [24122571,\\n          ...,\\n          24121481],\\n &#39;score&#39;: 412,\\n &#39;time&#39;: 1597154451,\\n &#39;title&#39;: &#39;Single Page Applications using Rust&#39;,\\n &#39;type&#39;: &#39;story&#39;,\\n &#39;url&#39;: &#39;http://www.sheshbabu.com/posts/rust-wasm-yew-single-page-application/&#39;}\\n{&#39;by&#39;: &#39;bmgoss&#39;,\\n &#39;descendants&#39;: 5,\\n &#39;id&#39;: 24123372,\\n &#39;kids&#39;: [24123579, 24124181, 24123545, 24123929],\\n &#39;score&#39;: 55,\\n &#39;time&#39;: 1597168165,\\n &#39;title&#39;: &#39;Classic Books for Tech Leads (or those aspiring to be)&#39;,\\n &#39;type&#39;: &#39;story&#39;,\\n &#39;url&#39;: &#39;https://sourcelevel.io/blog/3-classic-books-for-tech-leads-or-those-aspiring-to-be&#39;}\\n{&#39;by&#39;: &#39;adamnemecek&#39;,\\n &#39;descendants&#39;: 7,\\n &#39;id&#39;: 24123283,\\n &#39;kids&#39;: [24123803, 24123774, 24124106, 24123609],\\n &#39;score&#39;: 69,\\n &#39;time&#39;: 1597167845,\\n &#39;title&#39;: &#39;Bevy: Simple, data-driven, wgpu-based game engine in Rust&#39;,\\n &#39;type&#39;: &#39;story&#39;,\\n &#39;url&#39;: &#39;https://bevyengine.org&#39;}\\n</code></pre><p> If you found an interesting articles and wanted to dig a little deeper, then HackerNews API can help with that too. You can find comments of each submission by traversing <code class=\"inline\">kids</code> field of said story. Code that would do just that looks like so: </p><pre><code class=\"language-python\">\\nfirst = 24120311  # Top story\\nquery_url = f&quot;https://hacker-news.firebaseio.com/v0/item/{first}.json?print=pretty&quot;\\nr = requests.get(query_url)\\ncomment_ids = r.json()[&quot;kids&quot;]  # IDs of top level comments of first story\\n\\nfor i in comment_ids[:10]:  # Print first 10 comments of story\\n    query_url = f&quot;https://hacker-news.firebaseio.com/v0/item/{i}.json?print=pretty&quot;\\n    r = requests.get(query_url)\\n    pprint(r.json())\\n</code></pre><p> First, we look up story (<code class=\"inline\">item</code>) by ID like we did in previous example. We then iterate over its <code class=\"inline\">kids</code> and run same query with respective IDs retrieving items that in this case refer to story comments. We could also go through these recursively if we wanted to build whole tree/thread of comments of specific story. </p><p> As always, here is sample response: </p><pre><code class=\"language-python\">\\n{&#39;by&#39;: &#39;Naac&#39;,\\n &#39;id&#39;: 24123455,\\n &#39;kids&#39;: [24123485],\\n &#39;parent&#39;: 24120311,\\n &#39;text&#39;: &#39;So as I understand it Rust is compelling because it is a safer &#39;\\n         &#39;alternative to C++ ( and sometimes C but mainly a C++ replacement &#39;\\n         &#39;).&lt;p&gt;We wouldn&#39;t usually create a single page app in C++ right? &#39;\\n         &#39;So why would we want to do that in Rust ( other than, &quot;just &#39;\\n         &#39;because&quot; ). Right tool for the right job and all that.&#39;,\\n &#39;time&#39;: 1597168558,\\n &#39;type&#39;: &#39;comment&#39;}\\n{&#39;by&#39;: &#39;intelleak&#39;,\\n &#39;id&#39;: 24123860,\\n &#39;parent&#39;: 24120311,\\n &#39;text&#39;: &#39;I&#39;ve been hearing good things about zig, and someone mentioned &#39;\\n         &#39;that zig has better wasm support than rust, is it true? I wish rust &#39;\\n         &#39;had a js ecosystem too ...&#39;,\\n &#39;time&#39;: 1597170320,\\n &#39;type&#39;: &#39;comment&#39;}\\n{&#39;by&#39;: &#39;praveenperera&#39;,\\n &#39;id&#39;: 24120642,\\n &#39;kids&#39;: [24120867, 24120738, 24120940, 24120721],\\n &#39;parent&#39;: 24120311,\\n &#39;text&#39;: &#39;Great post.&lt;p&gt;I&#39;d love to see one talking about building a full &#39;\\n         &#39;stack app using Yew and Actix (or Rocket). And good ways of sharing &#39;\\n         &#39;types between the frontend and the backend.&#39;,\\n &#39;time&#39;: 1597156315,\\n &#39;type&#39;: &#39;comment&#39;}\\n{&#39;by&#39;: &#39;devxpy&#39;,\\n &#39;id&#39;: 24122583,\\n &#39;kids&#39;: [24122721, 24122756, 24122723],\\n &#39;parent&#39;: 24120311,\\n &#39;text&#39;: &#39;Can anyone please tell me how the author able to use html syntax in &#39;\\n         &#39;rust?&lt;p&gt;I get that there are macros, but how are html tags valid &#39;\\n         &#39;syntax? Is rust just interpreting the html content as &#39;\\n         &#39;strings?&lt;p&gt;I&#39;ve only ever seen C macros, and I don&#39;t &#39;\\n         &#39;remember seeing\\\\n&#39;\\n         &#39; this kind of wizardry happening there.&#39;,\\n &#39;time&#39;: 1597165060,\\n &#39;type&#39;: &#39;comment&#39;}\\n</code></pre><h2>Currents</h2><p> Finding popular and good quality news API is quite difficult as most classic newspapers don&#39;t have free public API. There are however, sources of aggregate news data which can be used to get articles and news from newspapers like for example <i>Financial Times</i> and <i>Bloomberg</i> which only provide paid API services or like <i>CNN</i> doesn&#39;t expose any API at all. </p><p> One of these aggregators is called <a href=\"https://currentsapi.services/en\"><i>Currents API</i></a>. It aggregates data from thousands of sources, 18 languages and over 70 countries and it&#39;s also free. </p><p> It&#39;s similar to the APIs shown before. We again need to first get API key. To do so, you need to register at <a href=\"https://currentsapi.services/en/register\">https://currentsapi.services/en/register</a>. After that, go to your profile at <a href=\"https://currentsapi.services/en/profile\">https://currentsapi.services/en/profile</a> and retrieve your API token. </p><p> With key (token) ready we can request some data. There&#39;s really just one interesting endpoint and that&#39;s <a href=\"https://api.currentsapi.services/v1/search\">https://api.currentsapi.services/v1/search</a>: </p><pre><code class=\"language-python\">\\n# https://currentsapi.services/en/docs/search\\napikey = os.getenv(&#39;CURRENTS_APIKEY&#39;, &#39;...&#39;)\\ncategory = &quot;business&quot;\\nlanguage = languages[&#39;English&#39;]  # Mapping from Language to Code, e.g.: &quot;English&quot;: &quot;en&quot;\\ncountry = regions[&quot;Canada&quot;]  # Mapping from Country to Code, e.g.: &quot;Canada&quot;: &quot;CA&quot;,\\nkeywords = &quot;bitcoin&quot;\\nt = &quot;1&quot;  # 1 for news, 2 for article and 3 for discussion content\\ndomain = &quot;financialpost.com&quot;  # website primary domain name (without www or blog prefix)\\nstart_date = &quot;2020-06-01T14:30&quot;  # YYYY-MM-DDTHH:MM:SS+00:00\\nquery_url = f&quot;https://api.currentsapi.services/v1/search?&quot; \\\\\\n            f&quot;apiKey={apikey}&quot; \\\\\\n            f&quot;&amp;language={language}&quot; \\\\\\n            f&quot;&amp;category={category}&quot; \\\\\\n            f&quot;&amp;country={country}&quot; \\\\\\n            f&quot;&amp;type={t}&quot; \\\\\\n            f&quot;&amp;domain={domain}&quot; \\\\\\n            f&quot;&amp;keywords={keywords}&quot; \\\\\\n            f&quot;&amp;start_date={start_date}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> This endpoint includes lots of filtering options including language, category, country and more, as shown in the snippet above. All of those are pretty self-explanatory, but for those first three I mentioned, you will need some extra information as their possible values aren&#39;t really obvious. These values come from API endpoints available <a href=\"https://currentsapi.services/api/docs/\">here</a> and in case of languages and regions are really just mappings of value to code (e.g. <code class=\"inline\">&quot;English&quot;: &quot;en&quot;</code>) and in case of categories just a list of possible values. It&#39;s omitted in the code above to make it a bit shorter, but I just copied these mappings into Python <code class=\"inline\">dict</code>s to avoid calling API every time. </p><p> Response to above request lands the following: </p><pre><code class=\"language-python\">\\n{&#39;news&#39;: [{&#39;author&#39;: &#39;Bloomberg News&#39;,\\n           &#39;category&#39;: [&#39;business&#39;],\\n           &#39;description&#39;: &#39;(Bloomberg)  Bitcoin is notoriously volatile, prone to sudden price surges and swift reversals &#39;\\n                          &#39;that can wipe out millions of dollars of value in a matter of minutes. Those changes are often...&#39;,\\n           &#39;id&#39;: &#39;cb50963e-73d6-4a21-bb76-ec8bc8b9c201&#39;,\\n           &#39;image&#39;: &#39;https://financialpostcom.files.wordpress.com/2017/11/fp-512x512.png&#39;,\\n           &#39;language&#39;: &#39;ru&#39;,\\n           &#39;published&#39;: &#39;2020-04-25 05:02:50 +0000&#39;,\\n           &#39;title&#39;: &#39;Get Set for Bitcoin Halving! Heres What That Means&#39;,\\n           &#39;url&#39;: &#39;https://business.financialpost.com/pmn/business-pmn/get-set-for-bitcoin-halving-heres-what-that-means&#39;},\\n          {&#39;author&#39;: &#39;Reuters&#39;,\\n           &#39;category&#39;: [&#39;business&#39;],\\n           &#39;description&#39;: &#39;NEW YORK  Crushing asset sell-offs ranging from bitcoin to precious metals and European stocks &#39;\\n                          &#39;accompanied Wall Streets slide into bear market territory on Thursday, as investors liqu&#39;,\\n           &#39;id&#39;: &#39;3c75b090-ec7d-423e-9487-85becd92d10c&#39;,\\n           &#39;image&#39;: &#39;https://financialpostcom.files.wordpress.com/2017/11/fp-512x512.png&#39;,\\n           &#39;language&#39;: &#39;en&#39;,\\n           &#39;published&#39;: &#39;2020-03-12 23:14:18 +0000&#39;,\\n           &#39;title&#39;: &#39;Wall Street sell-off batters bitcoin, pounds palladium as &#39;\\n                    &#39;investors go to cash&#39;,\\n           &#39;url&#39;: &#39;https://business.financialpost.com/pmn/business-pmn/wall-street-sell-off-batters-bitcoin-pounds-palladium-as-investors-go-to-cash&#39;}],\\n &#39;page&#39;: 1,\\n &#39;status&#39;: &#39;ok&#39;}\\n</code></pre><p> If you aren&#39;t searching for specific topic or historical data, then there&#39;s one other options which <i>Currents API</i> provides - the <i>latest news</i> endpoint: </p><pre><code class=\"language-python\">\\nlanguage = languages[&#39;English&#39;]\\nquery_url = f&quot;https://api.currentsapi.services/v1/latest-news?&quot; \\\\\\n            f&quot;apiKey={apikey}&quot; \\\\\\n            f&quot;&amp;language={language}&quot;\\n\\nr = requests.get(query_url)\\npprint(r.json())\\n</code></pre><p> It is very similar to the <code class=\"inline\">search</code> endpoint, this one however only provides <code class=\"inline\">language</code> parameter and produces results like this: </p><pre><code class=\"language-python\">\\n{&#39;news&#39;: [{&#39;author&#39;: &#39;Isaac Chotiner&#39;,\\n           &#39;category&#39;: [&#39;funny&#39;],\\n           &#39;description&#39;: &#39;The former U.S. Poet Laureate discusses her decision to tell her mother\\\\&#39;s story in prose, in &#39;\\n                          &#39;her new book, &quot;Memorial Drive,&quot; and her feelings about the destruction of Confederate monuments...&#39;,\\n           &#39;id&#39;: &#39;3ded3ed1-ecb8-41db-96d3-dc284f4a61de&#39;,\\n           &#39;image&#39;: &#39;https://media.newyorker.com/photos/5f330eba567fa2363b1a19c3/16:9/w_1280,c_limit/Chotiner-NatashaTrethewey.jpg&#39;,\\n           &#39;language&#39;: &#39;en&#39;,\\n           &#39;published&#39;: &#39;2020-08-12 19:15:03 +0000&#39;,\\n           &#39;title&#39;: &#39;How Natasha Trethewey Remembers Her Mother&#39;,\\n           &#39;url&#39;: &#39;https://www.newyorker.com/culture/q-and-a/how-natasha-trethewey-remembers-her-mother&#39;},\\n          {&#39;author&#39;: &#39;@BBCNews&#39;,\\n           &#39;category&#39;: [&#39;regional&#39;],\\n           &#39;description&#39;: &#39;Firefighters are tackling the blaze that broke out in the engineering department at the university...&#39;,\\n           &#39;id&#39;: &#39;9e1f1ee2-8041-4864-8cca-0ffaedf9ae2b&#39;,\\n           &#39;image&#39;: &#39;https://ichef.bbci.co.uk/images/ic/1024x576/p08ngy6g.jpg&#39;,\\n           &#39;language&#39;: &#39;en&#39;,\\n           &#39;published&#39;: &#39;2020-08-12 18:37:48 +0000&#39;,\\n           &#39;title&#39;: &quot;Fire at Swansea University&#39;s Bay campus&quot;,\\n           &#39;url&#39;: &#39;https://www.bbc.co.uk/news/uk-wales-53759352&#39;}],\\n &#39;page&#39;: 1,\\n &#39;status&#39;: &#39;ok&#39;}\\n\\n</code></pre><h2>Conclusion</h2><p> There are many great news sites and online newspapers out there on the internet, but in most cases you won&#39;t be able to scrape their data or access them programmatically. The ones shown in this article are the rare few with nice API and free access that you can use for your next project whether it&#39;s some data science, machine learning or simple news aggregator. If you don&#39;t mind paying some money for news API, you might also consider using <a href=\"https://developer.ft.com/portal\">Financial Times</a> or <a href=\"https://www.bloomberg.com/professional/support/api-library/\">Bloomberg</a>. Apart from APIs you can also try scraping HTML and parsing the content yourself with something like <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">BeautifulSoup</a>. If you happen to find any other good source of news data, please let me know, so that I can add it to this list.  </p><!--]--></div></div></article><div class=\"navbar\" data-v-a3042193><span class=\"footer-action\" data-v-a3042193><a href=\"/blog/30\" data-v-a3042193><i class=\"icon-left-open\" href=\"/blog/30\" data-v-a3042193></i><span class=\"footer-action-text\" data-v-a3042193>Previous</span><span class=\"footer-action-text-short\" data-v-a3042193>Prev.</span></a></span><span class=\"footer-action\" data-v-a3042193><a href=\"/blog/32\" data-v-a3042193><i class=\"icon-right-open\" href=\"/blog/32\" data-v-a3042193></i><span class=\"footer-action-text\" data-v-a3042193>Next</span><span class=\"footer-action-text-short\" data-v-a3042193>Next</span></a></span><span class=\"footer-action\" data-v-a3042193><i id=\"top_icon\" class=\"icon-up-open\" href=\"#\" data-v-a3042193></i><span class=\"footer-action-text\" data-v-a3042193>Top</span><span class=\"footer-action-text-short\" data-v-a3042193>Top</span></span><span class=\"footer-action\" data-v-a3042193><a href=\"/subscribe\" class=\"\" data-v-a3042193><i class=\"icon-rss\" data-v-a3042193></i><span class=\"footer-action-text\" data-v-a3042193>Subscribe</span><span class=\"footer-action-text-short\" data-v-a3042193>Sub</span></a></span><span class=\"footer-action\" data-v-a3042193><a href=\"https://ko-fi.com/martinheinz\" rel=\"noopener noreferrer\" data-v-a3042193><i class=\"icon-cup\" data-v-a3042193></i><i class=\"icon-money\" data-v-a3042193></i><span class=\"footer-action-text\" data-v-a3042193>Tip Jar</span><span class=\"footer-action-text-short\" data-v-a3042193>Tips</span></a></span></div></div><div class=\"mailing-list\"><h2><i class=\"icon-paper-plane\"></i> Subscribe:</h2><div><form id=\"sub-mail\" method=\"post\" novalidate=\"true\"><!----><p class=\"form-field\"><input id=\"email\" type=\"email\" value=\"\" placeholder=\"E-mail Address\"></p><p class=\"form-field\"><input type=\"submit\" value=\"Submit\"></p><div></div></form></div></div><footer id=\"footer\"><div class=\"footer-left\"> Copyright  2023 Martin Heinz </div><div class=\"footer-right\"><nav><ul><!--[--><li><a href=\"/\">Home</a></li><li><a href=\"/contact\">Contact</a></li><li><a href=\"/subscribe\">Subscribe</a></li><li><a href=\"https://ko-fi.com/martinheinz\">Tip Jar</a></li><!--]--></ul></nav></div></footer></div></div></div><!--]--><div class=\"nuxt-progress\" style=\"display:none;width:0%;\" data-v-1c64d9bb></div></div></div><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l){e.id=c;e.created_at=a;e.updated_at=a;e.deleted_at=b;e.title=f;e.text=\"\\\\u003Cp\\\\u003E\\\\nWhether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles can be challenging though as you will have to go through quite a few hoops to get to the actual data - finding the right news sources, exploring their APIs, figuring out how to authenticate against them and finally scraping the data. That\\'s a lot of work and no fun.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nSo, to save you some time and get you started, here\\'s list of public news APIs that I was able to find, with explanation how authenticate against them, query them and most importantly examples for how to get all the data you need from them!\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Ch2\\\\u003ENY Times\\\\u003C\\\\u002Fh2\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nFirst and the best source of data is in my opinion \\\\u003Ci\\\\u003ENew Your Times\\\\u003C\\\\u002Fi\\\\u003E. To start using its API you need to create an account at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Faccounts\\\\u002Fcreate\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Faccounts\\\\u002Fcreate\\\\u003C\\\\u002Fa\\\\u003E and an application at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fmy-apps\\\\u002Fnew-app\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fmy-apps\\\\u002Fnew-app\\\\u003C\\\\u002Fa\\\\u003E. When creating the application you get to choose which APIs to activate - I recommend activating at least \\\\u003Ci\\\\u003EMost Popular\\\\u003C\\\\u002Fi\\\\u003E, \\\\u003Ci\\\\u003EArticle Search\\\\u003C\\\\u002Fi\\\\u003E, \\\\u003Ci\\\\u003ETop Stories\\\\u003C\\\\u002Fi\\\\u003E and \\\\u003Ci\\\\u003EArchive APIs\\\\u003C\\\\u002Fi\\\\u003E. When your application is created you will be presented with the key which you will use to interact all the selected APIs, so copy it and let\\'s start querying!\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe simplest query we can do with \\\\u003Ci\\\\u003ENY Times\\\\u003C\\\\u002Fi\\\\u003E API is look up for current top stories:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\nimport requests\\\\nimport os\\\\nfrom pprint import pprint\\\\n\\\\napikey = os.getenv(\\'NYTIMES_APIKEY\\', \\'...\\')\\\\n\\\\n# Top Stories:\\\\n# https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fdocs\\\\u002Ftop-stories-product\\\\u002F1\\\\u002Foverview\\\\nsection = \\\\\"science\\\\\"\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fapi.nytimes.com\\\\u002Fsvc\\\\u002Ftopstories\\\\u002Fv2\\\\u002F{section}.json?api-key={apikey}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe snippet above is very straightforward. We run a \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003EGET\\\\u003C\\\\u002Fcode\\\\u003E request against \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Etopstories\\\\u002Fv2\\\\u003C\\\\u002Fcode\\\\u003E endpoint supplying \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Esection\\\\u003C\\\\u002Fcode\\\\u003E name and our API key. Section in this case is \\\\u003Ci\\\\u003Escience\\\\u003C\\\\u002Fi\\\\u003E, but \\\\u003Ci\\\\u003ENY Times\\\\u003C\\\\u002Fi\\\\u003E provides a lot of other options here, e.g. fashion, health, sports or theater. Full list can be found \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fdocs\\\\u002Ftop-stories-product\\\\u002F1\\\\u002Foverview\\\\\"\\\\u003Ehere\\\\u003C\\\\u002Fa\\\\u003E. This specific request would produce response that would look something like this:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{ \\'last_updated\\': \\'2020-08-09T08:07:44-04:00\\',\\\\n \\'num_results\\': 25,\\\\n \\'results\\': [{\\'abstract\\': \\'New Zealand marked 100 days with no new reported \\'\\\\n                          \\'cases of local coronavirus transmission. France \\'\\\\n                          \\'will require people to wear masks in crowded \\'\\\\n                          \\'outdoor areas.\\',\\\\n              \\'byline\\': \\'\\',\\\\n              \\'created_date\\': \\'2020-08-09T08:00:12-04:00\\',\\\\n              \\'item_type\\': \\'Article\\',\\\\n              \\'multimedia\\': [{\\'caption\\': \\'\\',\\\\n                              \\'copyright\\': \\'The New York Times\\',\\\\n                              \\'format\\': \\'superJumbo\\',\\\\n                              \\'height\\': 1080,\\\\n                              \\'subtype\\': \\'photo\\',\\\\n                              \\'type\\': \\'image\\',\\\\n                              \\'url\\': \\'https:\\\\u002F\\\\u002Fstatic01.nyt.com\\\\u002Fimages\\\\u002F2020\\\\u002F08\\\\u002F03\\\\u002Fus\\\\u002Fus-briefing-promo-image-print\\\\u002Fus-briefing-promo-image-superJumbo.jpg\\',\\\\n                              \\'width\\': 1920},\\\\n                             ],\\\\n              \\'published_date\\': \\'2020-08-09T08:00:12-04:00\\',\\\\n              \\'section\\': \\'world\\',\\\\n              \\'short_url\\': \\'https:\\\\u002F\\\\u002Fnyti.ms\\\\u002F3gH9NXP\\',\\\\n              \\'title\\': \\'Coronavirus Live Updates: DeWine Stresses Tests \\'\\\\n                       \\'Value, Even After His False Positive\\',\\\\n              \\'uri\\': \\'nyt:\\\\u002F\\\\u002Farticle\\\\u002F27dd9f30-ad63-52fe-95ab-1eba3d6a553b\\',\\\\n              \\'url\\': \\'https:\\\\u002F\\\\u002Fwww.nytimes.com\\\\u002F2020\\\\u002F08\\\\u002F09\\\\u002Fworld\\\\u002Fcoronavirus-covid-19.html\\'},\\\\n             ]\\\\n }\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nNext and probably the most useful endpoint when you are trying to get some specific set of data is the \\\\u003Ci\\\\u003Earticle search\\\\u003C\\\\u002Fi\\\\u003E endpoint:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n# Article Search:\\\\n# https:\\\\u002F\\\\u002Fapi.nytimes.com\\\\u002Fsvc\\\\u002Fsearch\\\\u002Fv2\\\\u002Farticlesearch.json?q=&lt;QUERY&gt;&api-key=&lt;APIKEY&gt;\\\\n# Use - https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fdocs\\\\u002Farticlesearch-product\\\\u002F1\\\\u002Froutes\\\\u002Farticlesearch.json\\\\u002Fget to explore API\\\\n\\\\nquery = \\\\\"politics\\\\\"\\\\nbegin_date = \\\\\"20200701\\\\\"  # YYYYMMDD\\\\nfilter_query = \\\\\"\\\\\\\\\\\\\"body:(\\\\\\\\\\\\\"Trump\\\\\\\\\\\\\") AND glocations:(\\\\\\\\\\\\\"WASHINGTON\\\\\\\\\\\\\")\\\\\\\\\\\\\"\\\\\"  # http:\\\\u002F\\\\u002Fwww.lucenetutorial.com\\\\u002Flucene-query-syntax.html\\\\npage = \\\\\"0\\\\\"  # &lt;0-100&gt;\\\\nsort = \\\\\"relevance\\\\\"  # newest, oldest\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fapi.nytimes.com\\\\u002Fsvc\\\\u002Fsearch\\\\u002Fv2\\\\u002Farticlesearch.json?\\\\\" \\\\\\\\\\\\n            f\\\\\"q={query}\\\\\" \\\\\\\\\\\\n            f\\\\\"&api-key={apikey}\\\\\" \\\\\\\\\\\\n            f\\\\\"&begin_date={begin_date}\\\\\" \\\\\\\\\\\\n            f\\\\\"&fq={filter_query}\\\\\" \\\\\\\\\\\\n            f\\\\\"&page={page}\\\\\" \\\\\\\\\\\\n            f\\\\\"&sort={sort}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThis endpoint features lots of filtering options. The only mandatory field is \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eq\\\\u003C\\\\u002Fcode\\\\u003E (query), which is the search term. Beyond that you can mix and match filter query, date range (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ebegin_date\\\\u003C\\\\u002Fcode\\\\u003E, \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eend_date\\\\u003C\\\\u002Fcode\\\\u003E), page number, sort order and facet fields. The filter query (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Efq\\\\u003C\\\\u002Fcode\\\\u003E) is interesting one, as it allows use of \\\\u003Ci\\\\u003ELucene query syntax\\\\u003C\\\\u002Fi\\\\u003E, which can be used to create complex filters with logical operators (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003EAND\\\\u003C\\\\u002Fcode\\\\u003E, \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003EOR\\\\u003C\\\\u002Fcode\\\\u003E), negations or wildcards. Nice tutorial can be found \\\\u003Ca href=\\\\\"http:\\\\u002F\\\\u002Fwww.lucenetutorial.com\\\\u002Flucene-query-syntax.html\\\\\"\\\\u003Ehere\\\\u003C\\\\u002Fa\\\\u003E.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nExample response for above query might like this (some fields were removed for clarity): \\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'response\\': {\\'docs\\': [{\\'_id\\': \\'nyt:\\\\u002F\\\\u002Farticle\\\\u002F0bf06be1-6699-527f-acb0-09fdd8abb6f6\\',\\\\n                        \\'abstract\\': \\'The president sidestepped Congress when it became clear that his nominee for a \\'\\\\n                                    \\'top Defense Department position would not win Senate approval.\\',\\\\n                        \\'byline\\': {\\'original\\': \\'By Helene Cooper\\'},\\\\n                        \\'document_type\\': \\'article\\',\\\\n                        \\'headline\\': {\\'main\\': \\'Trump Puts Pentagon in Political Crossfire With Tata Appointment\\',\\\\n                                     \\'print_headline\\': \\'Bypassing Congress to Appoint Ally, Trump Puts Pentagon in Political Crossfire\\'},\\\\n                        \\'keywords\\': [{\\'major\\': \\'N\\', \\'name\\': \\'subject\\', \\'rank\\': 1,\\\\n                                      \\'value\\': \\'United States Politics and Government\\'},\\\\n                                     {\\'major\\': \\'N\\', \\'name\\': \\'subject\\', \\'rank\\': 2,\\\\n                                      \\'value\\': \\'Appointments and Executive Changes\\'},\\\\n                                     {\\'major\\': \\'N\\', \\'name\\': \\'subject\\', \\'rank\\': 3,\\\\n                                      \\'value\\': \\'Presidential Election of 2020\\'}],\\\\n                        \\'lead_paragraph\\': \\'WASHINGTON  In making an end run around Congress to appoint Anthony J. Tata, a retired brigadier \\'\\\\n                                          \\'general with a history of Islamophobic and other inflammatory views, to a top Defense Department \\'\\\\n                                          \\'post, President Trump has once again put the military exactly where it does not want to be: in \\'\\\\n                                          \\'the middle of a political battle that could hurt bipartisan support for the Pentagon.\\',\\\\n                        \\'multimedia\\': [],\\\\n                        \\'news_desk\\': \\'Washington\\',\\\\n                        \\'pub_date\\': \\'2020-08-03T21:19:00+0000\\',\\\\n                        \\'section_name\\': \\'U.S.\\',\\\\n                        \\'source\\': \\'The New York Times\\',\\\\n                        \\'subsection_name\\': \\'Politics\\',\\\\n                        \\'type_of_material\\': \\'News\\',\\\\n                        \\'uri\\': \\'nyt:\\\\u002F\\\\u002Farticle\\\\u002F0bf06be1-6699-527f-acb0-09fdd8abb6f6\\',\\\\n                        \\'web_url\\': \\'https:\\\\u002F\\\\u002Fwww.nytimes.com\\\\u002F2020\\\\u002F08\\\\u002F03\\\\u002Fus\\\\u002Fpolitics\\\\u002Ftata-pentagon.html\\',\\\\n                        \\'word_count\\': 927}]}}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nLast endpoint for \\\\u003Ci\\\\u003ENY Times\\\\u003C\\\\u002Fi\\\\u003E that I will show here is their \\\\u003Ci\\\\u003EArchive API\\\\u003C\\\\u002Fi\\\\u003E which returns list of articles for given month going back all the way to 1851! This can be very useful if you need bulk data and don\\'t really need to search for specific terms.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n# Archive Search\\\\n# https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fdocs\\\\u002Farchive-product\\\\u002F1\\\\u002Foverview\\\\n\\\\nyear = \\\\\"1852\\\\\"  # &lt;1851 - 2020&gt;\\\\nmonth = \\\\\"6\\\\\"  # &lt;1 - 12&gt;\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fapi.nytimes.com\\\\u002Fsvc\\\\u002Farchive\\\\u002Fv1\\\\u002F{year}\\\\u002F{month}.json?api-key={apikey}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe query above searches for all articles from June of 1852 and from the result below we can see that even though we search for really old articles we still got 1888 hits. That said, most of these lack most of the useful data like keywords, word counts, author, etc. so you are probably better off searching for little more recent articles. \\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'response\\': {\\\\n        \\'meta\\': {\\'hits\\': 1888},\\\\n        \\'docs\\': [{\\'_id\\': \\'nyt:\\\\u002F\\\\u002Farticle\\\\u002Ffada2905-0108-54a9-8729-ae9cda8b9528\\',\\\\n                        \\'byline\\': {\\'organization\\': None, \\'original\\': None, \\'person\\': []},\\\\n                        \\'document_type\\': \\'article\\',\\\\n                        \\'headline\\': {\\'content_kicker\\': None, \\'kicker\\': \\'1\\',\\\\n                                     \\'main\\': \\'Sentence for Manslaughter.\\',\\\\n                                     \\'name\\': None,\\\\n                                     \\'print_headline\\': \\'Sentence for Manslaughter.\\'},\\\\n                        \\'keywords\\': [], \\'news_desk\\': \\'None\\',\\\\n                        \\'print_page\\': \\'3\\',\\\\n                        \\'pub_date\\': \\'1852-06-29T05:00:00+0000\\',\\\\n                        \\'section_name\\': \\'Archives\\',\\\\n                        \\'source\\': \\'The New York Times\\',\\\\n                        \\'type_of_material\\': \\'Archives\\',\\\\n                        \\'uri\\': \\'nyt:\\\\u002F\\\\u002Farticle\\\\u002Ffada2905-0108-54a9-8729-ae9cda8b9528\\',\\\\n                        \\'web_url\\': \\'https:\\\\u002F\\\\u002Fwww.nytimes.com\\\\u002F1852\\\\u002F06\\\\u002F29\\\\u002Farchives\\\\u002Fsentence-for-manslaughter.html\\',\\\\n                        \\'word_count\\': 0},\\\\n                ...]}\\\\n}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThese were just some of the (in my opinion) more useful APIs provided by \\\\u003Ci\\\\u003ENY Times\\\\u003C\\\\u002Fi\\\\u003E. Beside these, there are bunch more available at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fapis\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fapis\\\\u003C\\\\u002Fa\\\\u003E. To explore each API, I would also recommend playing with query builder like \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.nytimes.com\\\\u002Fdocs\\\\u002Farticlesearch-product\\\\u002F1\\\\u002Froutes\\\\u002Farticlesearch.json\\\\u002Fget\\\\\"\\\\u003Ethe one for article search\\\\u003C\\\\u002Fa\\\\u003E, which lets you and build and execute your test query right on the website without any coding.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Ch2\\\\u003EThe Guardian\\\\u003C\\\\u002Fh2\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nNext up is another great source of news and articles - \\\\u003Ci\\\\u003EThe Guardian\\\\u003C\\\\u002Fi\\\\u003E. Same as with \\\\u003Ci\\\\u003ENY Times\\\\u003C\\\\u002Fi\\\\u003E, we first need to sign up for an API key. You can do so at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fbonobo.capi.gutools.co.uk\\\\u002Fregister\\\\u002Fdeveloper\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fbonobo.capi.gutools.co.uk\\\\u002Fregister\\\\u002Fdeveloper\\\\u003C\\\\u002Fa\\\\u003E and you will receive your key in an email. With that out of the way, we can navigate to \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002F\\\\\"\\\\u003EAPI documentation\\\\u003C\\\\u002Fa\\\\u003E and start querying the API.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nLet\\'s start simply by querying content sections of \\\\u003Ci\\\\u003EThe Guardian\\\\u003C\\\\u002Fi\\\\u003E:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n# https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Fsection\\\\nquery = \\\\\"science\\\\\"\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fsections?\\\\\" \\\\\\\\\\\\n            f\\\\\"api-key={apikey}\\\\\" \\\\\\\\\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\n{\\'response\\': {\\'results\\': [{\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fscience\\',\\\\n                           \\'editions\\': [{\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fscience\\',\\\\n                                         \\'code\\': \\'default\\',\\\\n                                         \\'id\\': \\'science\\',\\\\n                                         \\'webTitle\\': \\'Science\\',\\\\n                                         \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Fscience\\'}],\\\\n                           \\'id\\': \\'science\\',\\\\n                           \\'webTitle\\': \\'Science\\',\\\\n                           \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Fscience\\'}],\\\\n              \\'status\\': \\'ok\\',\\\\n              \\'total\\': 1,\\\\n              \\'userTier\\': \\'developer\\'}}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThese sections group content into topics, which can be useful if you are looking for specific type of content, e.g. \\\\u003Ci\\\\u003Escience\\\\u003C\\\\u002Fi\\\\u003E or \\\\u003Ci\\\\u003Etechnology\\\\u003C\\\\u002Fi\\\\u003E. If we omit the query (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eq\\\\u003C\\\\u002Fcode\\\\u003E) parameter, we will instead receive full list of sections, which is about 75 records.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nMoving on to something little more interesting - searching by \\\\u003Ci\\\\u003Etags\\\\u003C\\\\u002Fi\\\\u003E:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n# https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Ftag\\\\nquery = \\\\\"weather\\\\\"\\\\nsection = \\\\\"news\\\\\"\\\\npage = \\\\\"1\\\\\"\\\\nquery_url = f\\\\\"http:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Ftags?\\\\\" \\\\\\\\\\\\n            f\\\\\"api-key={apikey}\\\\\" \\\\\\\\\\\\n            f\\\\\"&q={query}\\\\\" \\\\\\\\\\\\n            f\\\\\"&page={page}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\n{\\'response\\': {\\'currentPage\\': 1,\\\\n              \\'pageSize\\': 10,\\\\n              \\'pages\\': 139,\\\\n              \\'results\\': [\\\\n                          {\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Faustralia-news\\\\u002Faustralia-weather\\',\\\\n                           \\'id\\': \\'australia-news\\\\u002Faustralia-weather\\',\\\\n                           \\'sectionId\\': \\'australia-news\\',\\\\n                           \\'sectionName\\': \\'Australia news\\',\\\\n                           \\'type\\': \\'keyword\\',\\\\n                           \\'webTitle\\': \\'Australia weather\\',\\\\n                           \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Faustralia-news\\\\u002Faustralia-weather\\'},\\\\n                          {\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fworld\\\\u002Fextreme-weather\\',\\\\n                           \\'id\\': \\'world\\\\u002Fextreme-weather\\',\\\\n                           \\'sectionId\\': \\'world\\',\\\\n                           \\'sectionName\\': \\'World news\\',\\\\n                           \\'type\\': \\'keyword\\',\\\\n                           \\'webTitle\\': \\'Extreme weather\\',\\\\n                           \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Fworld\\\\u002Fextreme-weather\\'},\\\\n                          ],\\\\n              \\'startIndex\\': 1,\\\\n              \\'status\\': \\'ok\\',\\\\n              \\'total\\': 1385,\\\\n              \\'userTier\\': \\'developer\\'}}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThis query looks quite similar to the previous one and also returns similar kinds of data. Tags also group content into categories, but there are a lot more tags (around 50000) than sections. Each of these tags have structure like for example \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eworld\\\\u002Fextreme-weather\\\\u003C\\\\u002Fcode\\\\u003E. These are very useful when doing search for actual articles, which is what we will do next.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe one thing you really came here for is article search and for that we will use \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Fsearch\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Fsearch\\\\u003C\\\\u002Fa\\\\u003E:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\nquery = \\\\\"(hurricane OR storm)\\\\\"\\\\nquery_fields = \\\\\"body\\\\\"\\\\nsection = \\\\\"news\\\\\"  # https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Fsection\\\\ntag = \\\\\"world\\\\u002Fextreme-weather\\\\\"  # https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Ftag\\\\nfrom_date = \\\\\"2019-01-01\\\\\"\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fsearch?\\\\\" \\\\\\\\\\\\n            f\\\\\"api-key={apikey}\\\\\" \\\\\\\\\\\\n            f\\\\\"&q={query}\\\\\" \\\\\\\\\\\\n            f\\\\\"&query-fields={query_fields}\\\\\" \\\\\\\\\\\\n            f\\\\\"&section={section}\\\\\" \\\\\\\\\\\\n            f\\\\\"&tag={tag}\\\\\" \\\\\\\\\\\\n            f\\\\\"&from-date={from_date}\\\\\" \\\\\\\\\\\\n            f\\\\\"&show-fields=headline,byline,starRating,shortUrl\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe reason I first showed you \\\\u003Ci\\\\u003Esection\\\\u003C\\\\u002Fi\\\\u003E and \\\\u003Ci\\\\u003Etag\\\\u003C\\\\u002Fi\\\\u003E search is that those can be used in the article search. Above you can see that we used \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Esection\\\\u003C\\\\u002Fcode\\\\u003E and \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Etag\\\\u003C\\\\u002Fcode\\\\u003E parameters to narrow down our search, which values can be found using previously shown queries. Apart from these parameters, we also included the obvious \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eq\\\\u003C\\\\u002Fcode\\\\u003E parameter for our search query, but also starting date using \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Efrom-date\\\\u003C\\\\u002Fcode\\\\u003E as well as \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eshow-fields\\\\u003C\\\\u002Fcode\\\\u003E parameter, which allows us to request extra fields related to the content - in this case those would be headline, byline, rating and shortened URL. There\\'s bunch more of those with full list available \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fopen-platform.theguardian.com\\\\u002Fdocumentation\\\\u002Fsearch\\\\\"\\\\u003Ehere\\\\u003C\\\\u002Fa\\\\u003E.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nAnd as with all the previous ones, here is example response:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'response\\': {\\'currentPage\\': 1, \\'orderBy\\': \\'relevance\\', \\'pageSize\\': 10, \\'pages\\': 1,\\\\n              \\'results\\': [{\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fnews\\\\u002F2019\\\\u002Fdec\\\\u002F19\\\\u002Fweatherwatch-storms-hit-france-and-iceland-as-australia-overheats\\',\\\\n                           \\'fields\\': {\\'byline\\': \\'Daniel Gardner (MetDesk)\\',\\\\n                                      \\'headline\\': \\'Weatherwatch: storms hit France and Iceland as Australia overheats\\',\\\\n                                      \\'shortUrl\\': \\'https:\\\\u002F\\\\u002Fgu.com\\\\u002Fp\\\\u002Fdv4dq\\'},\\\\n                           \\'id\\': \\'news\\\\u002F2019\\\\u002Fdec\\\\u002F19\\\\u002Fweatherwatch-storms-hit-france-and-iceland-as-australia-overheats\\',\\\\n                           \\'pillarId\\': \\'pillar\\\\u002Fnews\\',\\\\n                           \\'sectionId\\': \\'news\\',\\\\n                           \\'type\\': \\'article\\',\\\\n                           \\'webPublicationDate\\': \\'2019-12-19T11:33:52Z\\',\\\\n                           \\'webTitle\\': \\'Weatherwatch: storms hit France and \\'\\\\n                                       \\'Iceland as Australia overheats\\',\\\\n                           \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Fnews\\\\u002F2019\\\\u002Fdec\\\\u002F19\\\\u002Fweatherwatch-storms-hit-france-and-iceland-as-australia-overheats\\'},\\\\n                          {\\'apiUrl\\': \\'https:\\\\u002F\\\\u002Fcontent.guardianapis.com\\\\u002Fnews\\\\u002F2020\\\\u002Fjan\\\\u002F31\\\\u002Fweatherwatch-how-repeated-flooding-can-shift-levees\\',\\\\n                           \\'fields\\': {\\'byline\\': \\'David Hambling\\',\\\\n                                      \\'headline\\': \\'Weatherwatch: how repeated \\'\\\\n                                                  \\'flooding can shift levees\\',\\\\n                                      \\'shortUrl\\': \\'https:\\\\u002F\\\\u002Fgu.com\\\\u002Fp\\\\u002Fd755m\\'},\\\\n                           \\'id\\': \\'news\\\\u002F2020\\\\u002Fjan\\\\u002F31\\\\u002Fweatherwatch-how-repeated-flooding-can-shift-levees\\',\\\\n                           \\'pillarId\\': \\'pillar\\\\u002Fnews\\',\\\\n                           \\'sectionId\\': \\'news\\',\\\\n                           \\'type\\': \\'article\\',\\\\n                           \\'webPublicationDate\\': \\'2020-01-31T21:30:00Z\\',\\\\n                           \\'webTitle\\': \\'Weatherwatch: how repeated flooding can shift levees\\',\\\\n                           \\'webUrl\\': \\'https:\\\\u002F\\\\u002Fwww.theguardian.com\\\\u002Fnews\\\\u002F2020\\\\u002Fjan\\\\u002F31\\\\u002Fweatherwatch-how-repeated-flooding-can-shift-levees\\'}],\\\\n              \\'startIndex\\': 1, \\'status\\': \\'ok\\', \\'total\\': 7, \\'userTier\\': \\'developer\\'}}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\n\\\\u003Ch2\\\\u003EHackerNews\\\\u003C\\\\u002Fh2\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nFor more tech oriented source of news, one might turn to \\\\u003Ci\\\\u003EHackerNews\\\\u003C\\\\u002Fi\\\\u003E, which also has its public REST API. It\\'s documented on \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fgithub.com\\\\u002FHackerNews\\\\u002FAPI\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fgithub.com\\\\u002FHackerNews\\\\u002FAPI\\\\u003C\\\\u002Fa\\\\u003E. This API, as you will see, is in version \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ev0\\\\u003C\\\\u002Fcode\\\\u003E and is currently very bare-bones, meaning it doesn\\'t really provide specific endpoints to - for example - query articles, comments or users.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nBut even though it\\'s very basic it still provides all that\\'s necessary to, for example, get top stories:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\nquery_type = \\\\\"top\\\\\"  # top\\\\u002Fbest\\\\u002Fnew, also ask\\\\u002Fshow\\\\u002Fjob\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fhacker-news.firebaseio.com\\\\u002Fv0\\\\u002F{query_type}stories.json?print=pretty\\\\\"  # Top Stories\\\\nr = requests.get(query_url)\\\\nids = r.json()\\\\n\\\\ntop = ids[:10]\\\\nfor story in top:\\\\n    query_url = f\\\\\"https:\\\\u002F\\\\u002Fhacker-news.firebaseio.com\\\\u002Fv0\\\\u002Fitem\\\\u002F{story}.json?print=pretty\\\\\"\\\\n    r = requests.get(query_url)\\\\n    pprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThe snippet above is not nearly as obvious as the previous ones, so let\\'s look at it more closely. We first send request to API endpoint (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ev0\\\\u002Ftopstories\\\\u003C\\\\u002Fcode\\\\u003E), which doesn\\'t return top stories as you would expect, but really just their IDs. To get the actual stories we take these IDs (first 10 of them) and send requests to \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ev0\\\\u002Fitem\\\\u002F&lt;ID&gt;\\\\u003C\\\\u002Fcode\\\\u003E endpoint which returns data for each of these individual items, which in this case happens to be a story.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nYou surely noticed that the query URL was parametrized with \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Equery_type\\\\u003C\\\\u002Fcode\\\\u003E. That\\'s because, HackerNews API also has similar endpoints for all the top sections of the website, that being - ask, show, job or new.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nOne nice thing about this API is that it doesn\\'t require authenticate, so you don\\'t need to request API key and don\\'t need to worry about rate limiting like with the other ones. \\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nRunning this code would land response that looks something like this:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'by\\': \\'rkwz\\',\\\\n \\'descendants\\': 217,\\\\n \\'id\\': 24120311,\\\\n \\'kids\\': [24122571,\\\\n          ...,\\\\n          24121481],\\\\n \\'score\\': 412,\\\\n \\'time\\': 1597154451,\\\\n \\'title\\': \\'Single Page Applications using Rust\\',\\\\n \\'type\\': \\'story\\',\\\\n \\'url\\': \\'http:\\\\u002F\\\\u002Fwww.sheshbabu.com\\\\u002Fposts\\\\u002Frust-wasm-yew-single-page-application\\\\u002F\\'}\\\\n{\\'by\\': \\'bmgoss\\',\\\\n \\'descendants\\': 5,\\\\n \\'id\\': 24123372,\\\\n \\'kids\\': [24123579, 24124181, 24123545, 24123929],\\\\n \\'score\\': 55,\\\\n \\'time\\': 1597168165,\\\\n \\'title\\': \\'Classic Books for Tech Leads (or those aspiring to be)\\',\\\\n \\'type\\': \\'story\\',\\\\n \\'url\\': \\'https:\\\\u002F\\\\u002Fsourcelevel.io\\\\u002Fblog\\\\u002F3-classic-books-for-tech-leads-or-those-aspiring-to-be\\'}\\\\n{\\'by\\': \\'adamnemecek\\',\\\\n \\'descendants\\': 7,\\\\n \\'id\\': 24123283,\\\\n \\'kids\\': [24123803, 24123774, 24124106, 24123609],\\\\n \\'score\\': 69,\\\\n \\'time\\': 1597167845,\\\\n \\'title\\': \\'Bevy: Simple, data-driven, wgpu-based game engine in Rust\\',\\\\n \\'type\\': \\'story\\',\\\\n \\'url\\': \\'https:\\\\u002F\\\\u002Fbevyengine.org\\'}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nIf you found an interesting articles and wanted to dig a little deeper, then HackerNews API can help with that too. You can find comments of each submission by traversing \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ekids\\\\u003C\\\\u002Fcode\\\\u003E field of said story. Code that would do just that looks like so:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\nfirst = 24120311  # Top story\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fhacker-news.firebaseio.com\\\\u002Fv0\\\\u002Fitem\\\\u002F{first}.json?print=pretty\\\\\"\\\\nr = requests.get(query_url)\\\\ncomment_ids = r.json()[\\\\\"kids\\\\\"]  # IDs of top level comments of first story\\\\n\\\\nfor i in comment_ids[:10]:  # Print first 10 comments of story\\\\n    query_url = f\\\\\"https:\\\\u002F\\\\u002Fhacker-news.firebaseio.com\\\\u002Fv0\\\\u002Fitem\\\\u002F{i}.json?print=pretty\\\\\"\\\\n    r = requests.get(query_url)\\\\n    pprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nFirst, we look up story (\\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Eitem\\\\u003C\\\\u002Fcode\\\\u003E) by ID like we did in previous example. We then iterate over its \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Ekids\\\\u003C\\\\u002Fcode\\\\u003E and run same query with respective IDs retrieving items that in this case refer to story comments. We could also go through these recursively if we wanted to build whole tree\\\\u002Fthread of comments of specific story.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nAs always, here is sample response:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'by\\': \\'Naac\\',\\\\n \\'id\\': 24123455,\\\\n \\'kids\\': [24123485],\\\\n \\'parent\\': 24120311,\\\\n \\'text\\': \\'So as I understand it Rust is compelling because it is a safer \\'\\\\n         \\'alternative to C++ ( and sometimes C but mainly a C++ replacement \\'\\\\n         \\').&lt;p&gt;We wouldn&#x27;t usually create a single page app in C++ right? \\'\\\\n         \\'So why would we want to do that in Rust ( other than, &quot;just \\'\\\\n         \\'because&quot; ). Right tool for the right job and all that.\\',\\\\n \\'time\\': 1597168558,\\\\n \\'type\\': \\'comment\\'}\\\\n{\\'by\\': \\'intelleak\\',\\\\n \\'id\\': 24123860,\\\\n \\'parent\\': 24120311,\\\\n \\'text\\': \\'I&#x27;ve been hearing good things about zig, and someone mentioned \\'\\\\n         \\'that zig has better wasm support than rust, is it true? I wish rust \\'\\\\n         \\'had a js ecosystem too ...\\',\\\\n \\'time\\': 1597170320,\\\\n \\'type\\': \\'comment\\'}\\\\n{\\'by\\': \\'praveenperera\\',\\\\n \\'id\\': 24120642,\\\\n \\'kids\\': [24120867, 24120738, 24120940, 24120721],\\\\n \\'parent\\': 24120311,\\\\n \\'text\\': \\'Great post.&lt;p&gt;I&#x27;d love to see one talking about building a full \\'\\\\n         \\'stack app using Yew and Actix (or Rocket). And good ways of sharing \\'\\\\n         \\'types between the frontend and the backend.\\',\\\\n \\'time\\': 1597156315,\\\\n \\'type\\': \\'comment\\'}\\\\n{\\'by\\': \\'devxpy\\',\\\\n \\'id\\': 24122583,\\\\n \\'kids\\': [24122721, 24122756, 24122723],\\\\n \\'parent\\': 24120311,\\\\n \\'text\\': \\'Can anyone please tell me how the author able to use html syntax in \\'\\\\n         \\'rust?&lt;p&gt;I get that there are macros, but how are html tags valid \\'\\\\n         \\'syntax? Is rust just interpreting the html content as \\'\\\\n         \\'strings?&lt;p&gt;I&#x27;ve only ever seen C macros, and I don&#x27;t \\'\\\\n         \\'remember seeing\\\\\\\\n\\'\\\\n         \\' this kind of wizardry happening there.\\',\\\\n \\'time\\': 1597165060,\\\\n \\'type\\': \\'comment\\'}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\n\\\\u003Ch2\\\\u003ECurrents\\\\u003C\\\\u002Fh2\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nFinding popular and good quality news API is quite difficult as most classic newspapers don\\'t have free public API. There are however, sources of aggregate news data which can be used to get articles and news from newspapers like for example \\\\u003Ci\\\\u003EFinancial Times\\\\u003C\\\\u002Fi\\\\u003E and \\\\u003Ci\\\\u003EBloomberg\\\\u003C\\\\u002Fi\\\\u003E which only provide paid API services or like \\\\u003Ci\\\\u003ECNN\\\\u003C\\\\u002Fi\\\\u003E doesn\\'t expose any API at all.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nOne of these aggregators is called \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\\"\\\\u003E\\\\u003Ci\\\\u003ECurrents API\\\\u003C\\\\u002Fi\\\\u003E\\\\u003C\\\\u002Fa\\\\u003E. It aggregates data from thousands of sources, 18 languages and over 70 countries and it\\'s also free.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nIt\\'s similar to the APIs shown before. We again need to first get API key. To do so, you need to register at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\u002Fregister\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\u002Fregister\\\\u003C\\\\u002Fa\\\\u003E. After that, go to your profile at \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\u002Fprofile\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\u002Fprofile\\\\u003C\\\\u002Fa\\\\u003E and retrieve your API token.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nWith key (token) ready we can request some data. There\\'s really just one interesting endpoint and that\\'s \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fapi.currentsapi.services\\\\u002Fv1\\\\u002Fsearch\\\\\"\\\\u003Ehttps:\\\\u002F\\\\u002Fapi.currentsapi.services\\\\u002Fv1\\\\u002Fsearch\\\\u003C\\\\u002Fa\\\\u003E:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n# https:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fen\\\\u002Fdocs\\\\u002Fsearch\\\\napikey = os.getenv(\\'CURRENTS_APIKEY\\', \\'...\\')\\\\ncategory = \\\\\"business\\\\\"\\\\nlanguage = languages[\\'English\\']  # Mapping from Language to Code, e.g.: \\\\\"English\\\\\": \\\\\"en\\\\\"\\\\ncountry = regions[\\\\\"Canada\\\\\"]  # Mapping from Country to Code, e.g.: \\\\\"Canada\\\\\": \\\\\"CA\\\\\",\\\\nkeywords = \\\\\"bitcoin\\\\\"\\\\nt = \\\\\"1\\\\\"  # 1 for news, 2 for article and 3 for discussion content\\\\ndomain = \\\\\"financialpost.com\\\\\"  # website primary domain name (without www or blog prefix)\\\\nstart_date = \\\\\"2020-06-01T14:30\\\\\"  # YYYY-MM-DDTHH:MM:SS+00:00\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fapi.currentsapi.services\\\\u002Fv1\\\\u002Fsearch?\\\\\" \\\\\\\\\\\\n            f\\\\\"apiKey={apikey}\\\\\" \\\\\\\\\\\\n            f\\\\\"&language={language}\\\\\" \\\\\\\\\\\\n            f\\\\\"&category={category}\\\\\" \\\\\\\\\\\\n            f\\\\\"&country={country}\\\\\" \\\\\\\\\\\\n            f\\\\\"&type={t}\\\\\" \\\\\\\\\\\\n            f\\\\\"&domain={domain}\\\\\" \\\\\\\\\\\\n            f\\\\\"&keywords={keywords}\\\\\" \\\\\\\\\\\\n            f\\\\\"&start_date={start_date}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThis endpoint includes lots of filtering options including language, category, country and more, as shown in the snippet above. All of those are pretty self-explanatory, but for those first three I mentioned, you will need some extra information as their possible values aren\\'t really obvious. These values come from API endpoints available \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fcurrentsapi.services\\\\u002Fapi\\\\u002Fdocs\\\\u002F\\\\\"\\\\u003Ehere\\\\u003C\\\\u002Fa\\\\u003E and in case of languages and regions are really just mappings of value to code (e.g. \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003E\\\\\"English\\\\\": \\\\\"en\\\\\"\\\\u003C\\\\u002Fcode\\\\u003E) and in case of categories just a list of possible values. It\\'s omitted in the code above to make it a bit shorter, but I just copied these mappings into Python \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Edict\\\\u003C\\\\u002Fcode\\\\u003Es to avoid calling API every time.\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nResponse to above request lands the following:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'news\\': [{\\'author\\': \\'Bloomberg News\\',\\\\n           \\'category\\': [\\'business\\'],\\\\n           \\'description\\': \\'(Bloomberg)  Bitcoin is notoriously volatile, prone to sudden price surges and swift reversals \\'\\\\n                          \\'that can wipe out millions of dollars of value in a matter of minutes. Those changes are often...\\',\\\\n           \\'id\\': \\'cb50963e-73d6-4a21-bb76-ec8bc8b9c201\\',\\\\n           \\'image\\': \\'https:\\\\u002F\\\\u002Ffinancialpostcom.files.wordpress.com\\\\u002F2017\\\\u002F11\\\\u002Ffp-512x512.png\\',\\\\n           \\'language\\': \\'ru\\',\\\\n           \\'published\\': \\'2020-04-25 05:02:50 +0000\\',\\\\n           \\'title\\': \\'Get Set for Bitcoin Halving! Heres What That Means\\',\\\\n           \\'url\\': \\'https:\\\\u002F\\\\u002Fbusiness.financialpost.com\\\\u002Fpmn\\\\u002Fbusiness-pmn\\\\u002Fget-set-for-bitcoin-halving-heres-what-that-means\\'},\\\\n          {\\'author\\': \\'Reuters\\',\\\\n           \\'category\\': [\\'business\\'],\\\\n           \\'description\\': \\'NEW YORK  Crushing asset sell-offs ranging from bitcoin to precious metals and European stocks \\'\\\\n                          \\'accompanied Wall Streets slide into bear market territory on Thursday, as investors liqu\\',\\\\n           \\'id\\': \\'3c75b090-ec7d-423e-9487-85becd92d10c\\',\\\\n           \\'image\\': \\'https:\\\\u002F\\\\u002Ffinancialpostcom.files.wordpress.com\\\\u002F2017\\\\u002F11\\\\u002Ffp-512x512.png\\',\\\\n           \\'language\\': \\'en\\',\\\\n           \\'published\\': \\'2020-03-12 23:14:18 +0000\\',\\\\n           \\'title\\': \\'Wall Street sell-off batters bitcoin, pounds palladium as \\'\\\\n                    \\'investors go to cash\\',\\\\n           \\'url\\': \\'https:\\\\u002F\\\\u002Fbusiness.financialpost.com\\\\u002Fpmn\\\\u002Fbusiness-pmn\\\\u002Fwall-street-sell-off-batters-bitcoin-pounds-palladium-as-investors-go-to-cash\\'}],\\\\n \\'page\\': 1,\\\\n \\'status\\': \\'ok\\'}\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nIf you aren\\'t searching for specific topic or historical data, then there\\'s one other options which \\\\u003Ci\\\\u003ECurrents API\\\\u003C\\\\u002Fi\\\\u003E provides - the \\\\u003Ci\\\\u003Elatest news\\\\u003C\\\\u002Fi\\\\u003E endpoint:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\nlanguage = languages[\\'English\\']\\\\nquery_url = f\\\\\"https:\\\\u002F\\\\u002Fapi.currentsapi.services\\\\u002Fv1\\\\u002Flatest-news?\\\\\" \\\\\\\\\\\\n            f\\\\\"apiKey={apikey}\\\\\" \\\\\\\\\\\\n            f\\\\\"&language={language}\\\\\"\\\\n\\\\nr = requests.get(query_url)\\\\npprint(r.json())\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nIt is very similar to the \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Esearch\\\\u003C\\\\u002Fcode\\\\u003E endpoint, this one however only provides \\\\u003Ccode class=\\\\\"inline\\\\\"\\\\u003Elanguage\\\\u003C\\\\u002Fcode\\\\u003E parameter and produces results like this:\\\\n\\\\u003C\\\\u002Fp\\\\u003E\\\\n\\\\u003Cpre\\\\u003E\\\\u003Chighlight-code lang=\\\\\"python\\\\\"\\\\u003E\\\\n{\\'news\\': [{\\'author\\': \\'Isaac Chotiner\\',\\\\n           \\'category\\': [\\'funny\\'],\\\\n           \\'description\\': \\'The former U.S. Poet Laureate discusses her decision to tell her mother\\\\\\\\\\'s story in prose, in \\'\\\\n                          \\'her new book, \\\\\"Memorial Drive,\\\\\" and her feelings about the destruction of Confederate monuments...\\',\\\\n           \\'id\\': \\'3ded3ed1-ecb8-41db-96d3-dc284f4a61de\\',\\\\n           \\'image\\': \\'https:\\\\u002F\\\\u002Fmedia.newyorker.com\\\\u002Fphotos\\\\u002F5f330eba567fa2363b1a19c3\\\\u002F16:9\\\\u002Fw_1280,c_limit\\\\u002FChotiner-NatashaTrethewey.jpg\\',\\\\n           \\'language\\': \\'en\\',\\\\n           \\'published\\': \\'2020-08-12 19:15:03 +0000\\',\\\\n           \\'title\\': \\'How Natasha Trethewey Remembers Her Mother\\',\\\\n           \\'url\\': \\'https:\\\\u002F\\\\u002Fwww.newyorker.com\\\\u002Fculture\\\\u002Fq-and-a\\\\u002Fhow-natasha-trethewey-remembers-her-mother\\'},\\\\n          {\\'author\\': \\'@BBCNews\\',\\\\n           \\'category\\': [\\'regional\\'],\\\\n           \\'description\\': \\'Firefighters are tackling the blaze that broke out in the engineering department at the university...\\',\\\\n           \\'id\\': \\'9e1f1ee2-8041-4864-8cca-0ffaedf9ae2b\\',\\\\n           \\'image\\': \\'https:\\\\u002F\\\\u002Fichef.bbci.co.uk\\\\u002Fimages\\\\u002Fic\\\\u002F1024x576\\\\u002Fp08ngy6g.jpg\\',\\\\n           \\'language\\': \\'en\\',\\\\n           \\'published\\': \\'2020-08-12 18:37:48 +0000\\',\\\\n           \\'title\\': \\\\\"Fire at Swansea University\\'s Bay campus\\\\\",\\\\n           \\'url\\': \\'https:\\\\u002F\\\\u002Fwww.bbc.co.uk\\\\u002Fnews\\\\u002Fuk-wales-53759352\\'}],\\\\n \\'page\\': 1,\\\\n \\'status\\': \\'ok\\'}\\\\n\\\\n\\\\u003C\\\\u002Fhighlight-code\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\n\\\\n\\\\u003Ch2\\\\u003EConclusion\\\\u003C\\\\u002Fh2\\\\u003E\\\\n\\\\u003Cp\\\\u003E\\\\nThere are many great news sites and online newspapers out there on the internet, but in most cases you won\\'t be able to scrape their data or access them programmatically. The ones shown in this article are the rare few with nice API and free access that you can use for your next project whether it\\'s some data science, machine learning or simple news aggregator. If you don\\'t mind paying some money for news API, you might also consider using \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fdeveloper.ft.com\\\\u002Fportal\\\\\"\\\\u003EFinancial Times\\\\u003C\\\\u002Fa\\\\u003E or \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fwww.bloomberg.com\\\\u002Fprofessional\\\\u002Fsupport\\\\u002Fapi-library\\\\u002F\\\\\"\\\\u003EBloomberg\\\\u003C\\\\u002Fa\\\\u003E. Apart from APIs you can also try scraping HTML and parsing the content yourself with something like \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Fwww.crummy.com\\\\u002Fsoftware\\\\u002FBeautifulSoup\\\\u002Fbs4\\\\u002Fdoc\\\\u002F\\\\\"\\\\u003EBeautifulSoup\\\\u003C\\\\u002Fa\\\\u003E. If you happen to find any other good source of news data, please let me know, so that I can add it to this list. &#x1F642;\\\\n\\\\u003C\\\\u002Fp\\\\u003E\";e.author=g;e.next=b;e.next_post_id=h;e.previous=b;e.previous_post_id=i;e.posted_on=j;e.sections=[{id:242,created_at:a,updated_at:a,deleted_at:b,post_id:c,name:\"NY Times\"},{id:243,created_at:a,updated_at:a,deleted_at:b,post_id:c,name:\"The Guardian\"},{id:244,created_at:a,updated_at:a,deleted_at:b,post_id:c,name:\"HackerNews\"},{id:245,created_at:a,updated_at:a,deleted_at:b,post_id:c,name:\"Currents\"},{id:246,created_at:a,updated_at:a,deleted_at:b,post_id:c,name:\"Conclusion\"}];e.tags=k;k[0]={id:122,created_at:a,updated_at:a,deleted_at:b,post_id:c,project_id:l,name:\"Python\"};k[1]={id:123,created_at:a,updated_at:a,deleted_at:b,post_id:c,project_id:l,name:\"API\"};return {data:{\"0GgTyKCieT\":e},state:{$scurrentPost:e,$scurrentPostHeader:{title:f,author:g,published:j,tags:k,description:\"\\\\u003Cp\\\\u003E\\\\nWhether you are data scientist, programmer or AI specialist, you surely can put huge number of news articles to some good use. Getting those articles c...\"},$scurrentPostLDJson:\"{\\\\\"@context\\\\\":\\\\\"http:\\\\u002F\\\\u002Fschema.org\\\\\",\\\\\"@type\\\\\":\\\\\"BlogPosting\\\\\",\\\\\"headline\\\\\":\\\\\"Scraping News and Articles From Public APIs with Python\\\\\",\\\\\"description\\\\\":\\\\\"Scraping News and Articles From Public APIs with Python\\\\\",\\\\\"image\\\\\":\\\\\"\\\\u002Ffavicon.ico\\\\\",\\\\\"url\\\\\":\\\\\"https:\\\\u002F\\\\u002Fmartinheinz.dev\\\\u002Fblog\\\\u002F31\\\\\",\\\\\"datePublished\\\\\":\\\\\"2020-08-20T17:30:00Z\\\\\",\\\\\"dateModified\\\\\":\\\\\"2020-08-20T17:30:00Z\\\\\",\\\\\"mainEntityOfPage\\\\\":{\\\\\"@type\\\\\":\\\\\"WebPage\\\\\"},\\\\\"author\\\\\":{\\\\\"@type\\\\\":\\\\\"Person\\\\\",\\\\\"name\\\\\":\\\\\"Martin Heinz\\\\\",\\\\\"url\\\\\":\\\\\"https:\\\\u002F\\\\u002Fmartinheinz.dev\\\\\"},\\\\\"publisher\\\\\":{\\\\\"@type\\\\\":\\\\\"Organization\\\\\",\\\\\"name\\\\\":\\\\\"Martin Heinz\\\\\",\\\\\"url\\\\\":\\\\\"https:\\\\u002F\\\\u002Fmartinheinz.dev\\\\\",\\\\\"logo\\\\\":{\\\\\"@type\\\\\":\\\\\"ImageObject\\\\\",\\\\\"url\\\\\":\\\\\"\\\\u002Ffavicon.ico\\\\\",\\\\\"width\\\\\":\\\\\"32\\\\\",\\\\\"height\\\\\":\\\\\"32\\\\\"}}}\",$spreviousPostId:i,$snextPostId:h,$spreviousPostExists:d,$snextPostExists:d},_errors:{},serverRendered:d,config:{public:{API_URL:\"https:\\\\u002F\\\\u002Fmartinheinz.dev:8080\\\\u002Fapi\\\\u002Fv1\\\\u002F\",HOST_NAME:\"martinheinz.dev\",UMAMI_TRACKING_ID:\"6261f15d-2344-48f8-aac1-aae795008ca9\"},app:{baseURL:\"\\\\u002F\",buildAssetsDir:\"\\\\u002F_nuxt\\\\u002F\",cdnURL:\"\"}}}}(\"0001-01-01T00:00:00Z\",null,31,true,{},\"Scraping News and Articles From Public APIs with Python\",\"Martin\",32,30,\"2020-08-20T17:30:00Z\",Array(2),0))</script><script type=\"module\" src=\"/_nuxt/entry.bb8b06e9.js\" crossorigin></script><script type=\"module\" src=\"/_nuxt/_id_.5b43800d.js\" crossorigin></script></body>\\n</html>')\n"
     ]
    }
   ],
   "source": [
    "#  test your function here to receive credit\n",
    "test = retrieve_html('https://martinheinz.dev/blog/31')\n",
    "print(test)\n",
    "# (200, '<!DOCTYPE html>\\n<html > )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIKuR9zMOp7r"
   },
   "source": [
    "Now while this example might have been fun, we haven't yet done anything more than we could with a web browser. To really see the power of programmatically making web requests we will need to interact with an API. For the rest of this lab we will be working with the [Yelp API](https://www.yelp.com/developers/documentation/v3/get_started) and Yelp data (for an extensive data dump see their [Academic Dataset Challenge](https://www.yelp.com/dataset_challenge))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVPOlsawOp7r"
   },
   "source": [
    "### Yelp API Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAgdwqZlOp7r"
   },
   "source": [
    "The reasons for using the Yelp API are 3 fold:\n",
    "\n",
    "1. Incredibly rich dataset that combines:\n",
    "    * entity data (users and businesses)\n",
    "    * preferences (i.e. ratings)\n",
    "    * geographic data (business location and check-ins)\n",
    "    * temporal data\n",
    "    * text in the form of reviews\n",
    "    * and even images.\n",
    "2. Well [documented API](https://www.yelp.com/developers/documentation/v3/get_started) with thorough examples.\n",
    "3. Extensive data coverage so that you can find data that you know personally (from your home town/city or account). This will help with understanding and interpreting your results.\n",
    "\n",
    "Yelp used to use OAuth tokens but has now switched to API Keys. **For the sake of backwards compatibility Yelp still provides a Client ID and Secret for OAuth, but you will not need those for this assignment.**\n",
    "\n",
    "To access the Yelp API, we will need to go through a few more steps than we did with the first NYT example. Most large web scale companies use a combination of authentication and rate limiting to control access to their data to ensure that everyone using it abides. The first step (even before we make any request) is to setup a Yelp account if you do not have one and get API credentials.\n",
    "\n",
    "1. Create a [Yelp](https://www.yelp.com/login) account (if you do not have one already)\n",
    "2. [Generate API keys](https://www.yelp.com/developers/v3/manage_app) (if you haven't already). You will only need the API Key (not the Client ID or Client Secret) -- more on that later.\n",
    "\n",
    "Now that we have our accounts setup we can start making requests!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAbtXXEnOp7r"
   },
   "source": [
    "### Question 2.2: Authenticated HTTP Request with the Yelp API **(15%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkrVU29lOp7r"
   },
   "source": [
    "\n",
    "\n",
    "First, store your Yelp credentials in a local file (kept out of version control) which you can read in to authenticate with the API. This file can be any format/structure since you will fill in the function stub below.\n",
    "\n",
    "For example, you may want to store your key in a file called `yelp_api_key.txt` (run in terminal):\n",
    "```bash\n",
    "echo 'YOUR_YELP_API_KEY' > yelp_api_key.txt\n",
    "```\n",
    "if you are using google colab: Run this in a cell:\n",
    "```bash\n",
    "!echo 'YOUR_YELP_API_KEY' > yelp_api_key.txt\n",
    "```\n",
    "\n",
    "**KEEP THE API KEY FILE PRIVATE AND OUT OF VERSION CONTROL (and definitely do not submit them to Gradescope!)**\n",
    "\n",
    "You can then read from the file using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "uHaxcZc3Op7r"
   },
   "outputs": [],
   "source": [
    "with open('yelp_api_key.txt', 'r') as f:\n",
    "    api_key = f.read().replace('\\n','')\n",
    "#     print(api_key)\n",
    "    # verify your api_key is correct\n",
    "# DO NOT FORGET TO CLEAR THE OUTPUT TO KEEP YOUR API KEY PRIVATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "FdokIqqfOp7s"
   },
   "outputs": [],
   "source": [
    "def read_api_key(filepath):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "\n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "\n",
    "    # feel free to modify this function if you are storing the API Key differently\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD1Eil7VOp7s"
   },
   "source": [
    "Using the Yelp API, fill in the following function stub to make an authenticated request to the [search](https://www.yelp.com/developers/documentation/v3/business_search) endpoint. Remember Yelp allows you to pass the API Key via a special HTTP Header: `Authorization: Bearer <API_KEY>`. Check out the [docs](https://www.yelp.com/developers/documentation/v3/authentication) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "UxROtrgSOp7s"
   },
   "outputs": [],
   "source": [
    "def location_search_params(api_key, location, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct url, headers and url_params. Reference API docs (link above) to use the arguments\n",
    "    \"\"\"\n",
    "    # [YOUR CODE HERE]\n",
    "    # What is the url endpoint for search?\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "    # How is Authentication performed?\n",
    "    headers = {\n",
    "    \n",
    "        'Authorization':f'Bearer {api_key}'\n",
    "    \n",
    "    }\n",
    "    # SPACES in url is problematic. How should you handle location containing spaces?\n",
    "    url_params = {\n",
    "        \n",
    "        'location' : location,\n",
    "        \n",
    "    }\n",
    "    # Include keyword arguments in url_params\n",
    "\n",
    "    url_params.update(kwargs)\n",
    "    \n",
    "    \n",
    "    return url, headers, url_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tMF8BoFOp7s"
   },
   "source": [
    "Hint: `**kwargs` represent keyword arguments that are passed to the function. For example, if you called the function `location_search_params(api_key, location, offset=0, limit=50)`. The arguments `api_key` and `location` are called *positional arguments* and key-value pair arguments are called **keyword arguments**. Your `kwargs` variable will be a python dictionary with those keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "S35pV3yTOp7s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://api.yelp.com/v3/businesses/search',\n",
       " {'Authorization': 'Bearer test_api_key_xyz'},\n",
       " {'location': 'Chicago', 'offset': 0, 'limit': 50})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code here to receive credit.\n",
    "api_key = \"test_api_key_xyz\"\n",
    "location = \"Chicago\"\n",
    "url, headers, url_params = location_search_params(api_key, location, offset=0, limit=50)\n",
    "url, headers, url_params\n",
    "# ('https://<hidden_url_check_search_endpoint_docs_to_get_answer>',\n",
    "#  {'Authorization': 'Bearer test_api_key_xyz'},\n",
    "#  {'location': 'Chicago', 'offset': 0, 'limit': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyDT0VCnOp7s"
   },
   "source": [
    "Now use `location_search_params(api_key, location, **kwargs)` to actually search restaurants from Yelp API. Most of the code is provided to you. Complete the `api_get_request` function given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "6Ap0_Hc6Op7t",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "20\n",
      "['Girl & The Goat', 'The Purple Pig', 'Cafe Ba-Ba-Reeba!', 'Au Cheval', \"Bavette's Bar & Boeuf\", 'S.K.Y.', 'Aba', 'The Dearborn', 'Sapori Trattoria', 'Alinea', 'Ema', \"Joe's Seafood, Prime Steak & Stone Crab\", 'Little Bad Wolf', 'Boka', 'etta - Bucktown', 'KAI ZAN', \"Cindy's Rooftop\", 'Monteverde', 'Prime & Provisions', 'The Gage']\n"
     ]
    }
   ],
   "source": [
    "def api_get_request(url, headers, url_params):\n",
    "    \"\"\"\n",
    "    Send a HTTP GET request and return a json response\n",
    "\n",
    "    Args:\n",
    "        url (string): API endpoint url\n",
    "        headers (dict): A python dictionary containing HTTP headers including Authentication to be sent\n",
    "        url_params (dict): The parameters (required and optional) supported by endpoint\n",
    "\n",
    "    Returns:\n",
    "        results (json): response as json\n",
    "    \"\"\"\n",
    "    http_method = 'GET'\n",
    "    response = requests.get(url,headers = headers,params = url_params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def yelp_search(api_key, location, offset=0):\n",
    "    \"\"\"\n",
    "    Make an authenticated request to the Yelp API.\n",
    "\n",
    "    Args:\n",
    "        api_key (string): Your Yelp API Key for Authentication\n",
    "        location (string): Business Location\n",
    "        offset (int): param for pagination\n",
    "\n",
    "    Returns:\n",
    "        total (integer): total number of businesses on Yelp corresponding to the location\n",
    "        businesses (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    url, headers, url_params = location_search_params(api_key, location, offset=0)\n",
    "    response_json = api_get_request(url, headers, url_params)\n",
    "    return response_json[\"total\"], list(response_json[\"businesses\"])\n",
    "\n",
    "# test your code here to receive credit\n",
    "# [YOUR CODE HERE]\n",
    "with open('yelp_api_key.txt', 'r') as f:\n",
    "    api_key = f.read().replace('\\n','')\n",
    "\n",
    "num_records, data = yelp_search(api_key, 'Chicago')\n",
    "print(num_records)\n",
    "#9000\n",
    "print(len(data))\n",
    "#20\n",
    "print(list(map(lambda x: x['name'], data)))\n",
    "#['Girl & The Goat', 'Wildberry Pancakes and Cafe', 'Au Cheval', 'The Purple Pig', \"Lou Malnati's Pizzeria\", 'Art Institute of Chicago', \"Bavette's Bar & Boeuf\", 'Cafe Ba-Ba-Reeba!', 'Smoque BBQ', 'Little Goat Diner', \"Pequod's Pizzeria\", 'Alinea', 'Quartino Ristorante', \"Kuma's Corner - Belmont\", \"Joe's Seafood, Prime Steak & Stone Crab\", 'Crisp', \"Portillo's Hot Dogs\", 'Sapori Trattoria', 'Xoco', \"Molly's Cupcakes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN_ee4ruOp7t"
   },
   "source": [
    "### Parameterization and Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdo_wAeYOp7t"
   },
   "source": [
    "Now that we have completed the \"hello world\" of working with the Yelp API, we are ready to really fly! The rest of the exercise will have a bit less direction since there are a variety of ways to retrieve the requested information but you should have all the component knowledge at this point to work with the API. Yelp being a fairly general platform actually has many more business than just restaurants, but by using the flexibility of the API we can ask it to only return the restaurants.\n",
    "\n",
    "\n",
    "\n",
    "And before we can get any reviews on restaurants, we need to actually get the metadata on ALL of the restaurants in Chicago. Notice above that while Yelp told us that there are ~240, the response contained fewer actual `Business` objects. This is due to pagination and is a safeguard against returning __TOO__ much data in a single request (what would happen if there were 100,000 restaurants?) and can be used in conjuction with _rate limiting_ as well as a way to throttle and protect access to Yelp data.\n",
    "\n",
    "> As a thought exercise, consider: If an API has 1,000,000 records, but only returns 10 records per page and limits you to 5 requests per second... how long will it take to acquire ALL of the records contained in the API?\n",
    "\n",
    "One of the ways that APIs are an improvement over plain web scraping is the ability to make __parameterized__ requests. Just like the Python functions you have been writing have arguments (or parameters) that allow you to customize its behavior/actions (an output) without having to rewrite the function entirely, we can parameterize the queries we make to the Yelp API to filter the results it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWUwj7koOp7t"
   },
   "source": [
    "### Question 2.3: Acquire all of the restaurants in Chicago on Yelp **(10%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MN67OHJOp7t"
   },
   "source": [
    "\n",
    "\n",
    "Again using the [API documentation](https://www.yelp.com/developers/documentation/v3/business_search) for the `search` endpoint, fill in the following function to retrieve all of the _Restuarants_ (using categories) for a given query. Again you should use your `read_api_key()` function outside of the `all_restaurants()` stub to read the API Key used for the requests. You will need to account for __pagination__ and __[rate limiting](https://www.yelp.com/developers/faq)__ to:\n",
    "\n",
    "1. Retrieve all of the Business objects (# of business objects should equal `total` in the response). **Paginate by querying 10 restaurants each request.**\n",
    "2. Pause slightly (at least 200 milliseconds) between subsequent requests so as to not overwhelm the API (and get blocked).  \n",
    "\n",
    "As always with API access, make sure you follow all of the [API's policies](https://www.yelp.com/developers/api_terms) and use the API responsibly and respectfully.\n",
    "\n",
    "**DO NOT MAKE TOO MANY REQUESTS TOO QUICKLY OR YOUR KEY MAY BE BLOCKED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "_vvKCS0XOp7t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://api.yelp.com/v3/businesses/search',\n",
       "  {'Authorization': 'Bearer test_api_key_xyz'},\n",
       "  {'location': 'Chicago', 'offset': 0, 'limit': 10}),\n",
       " ('https://api.yelp.com/v3/businesses/search',\n",
       "  {'Authorization': 'Bearer test_api_key_xyz'},\n",
       "  {'location': 'Chicago', 'offset': 10, 'limit': 10})]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def paginated_restaurant_search_requests(api_key, location, total):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples (url, headers, url_params) for paginated search of all restaurants\n",
    "    Args:\n",
    "        api_key (string): Your Yelp API Key for Authentication\n",
    "        location (string): Business Location\n",
    "        total (int): Total number of items to be fetched\n",
    "    Returns:\n",
    "        results (list): list of tuple (url, headers, url_params)\n",
    "    \"\"\"\n",
    "    # HINT: Use total, offset and limit for pagination\n",
    "    # You can reuse function location_search_params(...)\n",
    "    # [YOUR CODE HERE]\n",
    "    \n",
    "    offset = 0\n",
    "    limit = 10\n",
    "    numLimit = math.ceil(total/limit)\n",
    "    cat = 'restaraunts'\n",
    "    paginatedResults = []\n",
    "  \n",
    "    \n",
    "    for x in range(numLimit):\n",
    "        parameters = {\n",
    "        'offset': offset,\n",
    "        'limit': limit,\n",
    "        'categories': cat,\n",
    "        }\n",
    "    \n",
    "        url,head,url_param = location_search_params(api_key,location,**parameters)\n",
    "        paginatedResults.append((url,head,url_param))\n",
    "        offset += limit\n",
    "        \n",
    "  \n",
    "\n",
    "    # returns url, headers, url_params\n",
    "    return paginatedResults\n",
    "\n",
    "\n",
    "# Test your code here to receive credit.\n",
    "\n",
    "# with open('yelp_api_key.txt', 'r') as f:\n",
    "#     api_key = f.read().replace('\\n','')\n",
    "api_key = \"test_api_key_xyz\"#replace this\n",
    "location = \"Chicago\"\n",
    "all_restaurants_requests = paginated_restaurant_search_requests(api_key, location, 15)\n",
    "all_restaurants_requests\n",
    "\n",
    "# [('https:<hidden>',\n",
    "#   {'Authorization': 'Bearer test_api_key_xyz'},\n",
    "#   {'location': 'Chicago',\n",
    "#    'offset': 0,\n",
    "#    'limit': 10,\n",
    "#    'categories': '<hidden>'}),\n",
    "#  ('https:<hidden>',\n",
    "#   {'Authorization': 'Bearer test_api_key_xyz'},\n",
    "#   {'location': 'Chicago',\n",
    "#    'offset': 10,\n",
    "#    'limit': 10,\n",
    "#    'categories': '<hidden>'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "T66ixejxOp7u"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def all_restaurants(api_key, location):\n",
    "    \"\"\"\n",
    "    Construct the pagination requests for ALL the restaurants on Yelp for a given location.\n",
    "\n",
    "    Args:\n",
    "        api_key (string): Your Yelp API Key for Authentication\n",
    "        location (string): Business Location\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicts representing each restaurant\n",
    "    \"\"\"\n",
    "    # What keyword arguments should you pass to get first page of restaurants in Yelp\n",
    "    url, headers, url_params = location_search_params(api_key, location, offset=0, limit=10)\n",
    "    #\n",
    "    response_json = api_get_request(url, headers, url_params)\n",
    "    total_items = response_json[\"total\"]\n",
    "\n",
    "    all_restaurants_request = paginated_restaurant_search_requests(api_key, location, total_items)\n",
    "\n",
    "    # Use returned list of (url, headers, url_params) and function api_get_request to retrive all restaurants\n",
    "    # REMEMBER to pause slightly after each request.\n",
    "    # [YOUR CODE HERE]\n",
    "    #     api_get_request returns a JSON file \n",
    "    \n",
    "    allRestaurants = []\n",
    "    \n",
    "    for x in all_restaurants_request:\n",
    "        url,headers,params = x\n",
    "        response_json = api_get_request(url, headers, params)\n",
    "        restairants = response_json.get(\"businesses\",[])\n",
    "        allRestaurants.extend(restairants)\n",
    "        #one second sleep just to be safe\n",
    "        sleep(.500)\n",
    "    return allRestaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1op9HIZtOp7u"
   },
   "source": [
    "You can test your function with an individual neighborhood in Chicago (for example, Greektown). Chicago itself has a lot of restaurants... meaning it will take a lot of time to download them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "4qVB5qrlOp7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "['Greek Islands Restaurant', 'Monteverde', 'Trivoli Tavern', 'Athena Greek Restaurant', 'Green Street Smoked Meats', 'CityBird', 'Artopolis', 'Sepia', \"Formento's\", '9 Muses', 'Viaggio Ristorante & Lounge', 'El Che Steakhouse & Bar', 'Tamashii Ramen', 'High Five Ramen', 'Rye Deli & Drink', 'The Allis', 'Meli Cafe & Juice Bar', 'Green Street Local', 'Zeus Restaurant', 'Sawada Coffee', 'Booze Box', 'Spectrum Bar and Grill', 'Primos Chicago Pizza', 'Xian Cuisine', 'Suenos x Soho House', 'Dawali Jerusalem Kitchen', \"Nando's Peri-Peri\", \"Philly's Best\", 'Mr Greek Gyros', 'Omakase Yume', 'Ciao! Cafe & Wine Lounge', \"Nancy's Pizza West Loop\", 'Parlor Pizza Bar', 'TenGoku Aburiya', 'Lolas Restaurant & Bar', 'Jubilee Juice & Grill', \"Giordano's\", 'J.P. Graziano Grocery', 'H Mart - Chicago', 'Blaze Pizza', 'Slightly Toasted', 'Stelios Bottles & Bites', 'Fox Bar', \"Dugan's\", \"Nonna's Pizza & Sandwiches\", 'La Colombe Coffee', 'Taco Lul', 'Taco Burrito King - Greektown', 'SGD Dubu So Gong Dong Tofu & Korean BBQ', 'Yolk West Loop', \"Lou Mitchell's\", 'Blackwood BBQ', 'Beggars Pizza', \"Wok N' Bao\", 'Vanille Patisserie', 'Dylans Tavern & Grill', 'Starbucks', 'Sushi Pink', 'Tacombi', 'Asadito', 'Dan the Baker', 'Meddle Coffee Bar', 'Mary Bartleme Park', 'National Hellenic Museum', 'Ground Up', 'Pockets', 'Polpetti Espresso Bar', 'Morgan Street Cafe', \"Jet's Pizza\", \"Stan's Donuts & Coffee\", 'I Dream of Falafel', 'Paris Baguette', \"JoKeR's Cajun Kitchen\", 'Naf Naf Grill', 'Good Night John Boy Chicago', 'Dessert Dealer', 'Goddess and Grocer', 'Greektown Chicago SSA #16', 'Subway', 'Izakaya yume', \"Domino's Pizza\", 'K-Kitchen', 'Chipotle Mexican Grill', 'Red Star Bar', \"Sang's Kitchen\", 'Klay Oven Kitchen', 'Oki Sushi', 'lli', 'West Loop Dog Park', \"Jimmy John's\", 'Starbucks', 'Panera Bread', 'Krispy Rice', 'Five Guys', 'Starbucks', 'Monday Coffee', 'Heritage Green Park', 'Crepe & Bean', 'Potbelly Sandwich Shop', \"Dunkin'\", \"Sam's Crispy Chicken - West Loop\", 'Potbelly Sandwich Shop', 'Paper Thin Pizza', 'The Batcolumn', 'Hardbitten', 'Great Steak', \"Harold's Chicken On Clinton\", 'Itsapop', 'Hunan House', 'Bays English Muffins', \"Jimmy John's\", 'Taco Bell Cantina', 'Baskin Robbins', \"Dunkin'\", 'Burger King', '7-Eleven', 'this little goat kitchen', 'Toreore', 'Chicago News Room', '7-Eleven', 'Cafe Italo', 'Marche', '7-Eleven', 'Adams-Sangamon Park', 'Bayou Express - Food Truck', 'Subway', 'Spaketeria', 'Flik International']\n"
     ]
    }
   ],
   "source": [
    "api_key = read_api_key('yelp_api_key.txt')\n",
    "data = all_restaurants(api_key, 'Greektown, Chicago, IL')\n",
    "print(len(data))\n",
    "# 92\n",
    "print(list(map(lambda x:x['name'], data)))\n",
    "# ['Greek Islands Restaurant', 'Artopolis', 'Meli Cafe & Juice Bar', 'Athena Greek Restaurant', 'WJ Noodles', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyi_BCOJOp7u"
   },
   "source": [
    "Now that we have the metadata on all of the restaurants in Greektown (or at least the ones listed on Yelp), we can retrieve the reviews and ratings. The Yelp API gives us aggregate information on ratings but it doesn't give us the review text or individual users' ratings for a restaurant. For that we need to turn to web scraping, but to find out what pages to scrape we first need to parse our JSON from the API to extract the URLs of the restaurants.\n",
    "\n",
    "In general, it is a best practice to separate the act of __downloading__ data and __parsing__ data. This ensures that your data processing pipeline is modular and extensible (and autogradable ;). This decoupling also solves the problem of expensive downloading but cheap parsing (in terms of computation and time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3VtTaqaOp7v"
   },
   "source": [
    "\n",
    "### Question 2.4: Parse the API Responses and Extract the URLs **(5%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp6RygetOp7y"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Because we want to separate the __downloading__ from the __parsing__, fill in the following function to parse the URLs pointing to the restaurants on `yelp.com`. As input your function should expect a string of [properly formatted JSON](http://www.json.org/) (which is similar to __BUT__ not the same as a Python dictionary) and as output should return a Python list of strings. Hint: print your `data` to see the JSON-formatted information you have. The input JSON will be structured as follows :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"total\": 8228,\n",
    "  \"businesses\": [\n",
    "    {\n",
    "      \"rating\": 4,\n",
    "      \"price\": \"$\",\n",
    "      \"phone\": \"+14152520800\",\n",
    "      \"id\": \"four-barrel-coffee-san-francisco\",\n",
    "      \"is_closed\": false,\n",
    "      \"categories\": [\n",
    "        {\n",
    "          \"alias\": \"coffee\",\n",
    "          \"title\": \"Coffee & Tea\"\n",
    "        }\n",
    "      ],\n",
    "      \"review_count\": 1738,\n",
    "      \"name\": \"Four Barrel Coffee\",\n",
    "      \"url\": \"https://www.yelp.com/biz/four-barrel-coffee-san-francisco\",\n",
    "      \"coordinates\": {\n",
    "        \"latitude\": 37.7670169511878,\n",
    "        \"longitude\": -122.42184275\n",
    "      },\n",
    "      \"image_url\": \"http://s3-media2.fl.yelpcdn.com/bphoto/MmgtASP3l_t4tPCL1iAsCg/o.jpg\",\n",
    "      \"location\": {\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"country\": \"US\",\n",
    "        \"address2\": \"\",\n",
    "        \"address3\": \"\",\n",
    "        \"state\": \"CA\",\n",
    "        \"address1\": \"375 Valencia St\",\n",
    "        \"zip_code\": \"94103\"\n",
    "      },\n",
    "      \"distance\": 1604.23,\n",
    "      \"transactions\": [\"pickup\", \"delivery\"]\n",
    "    }\n",
    "  ],\n",
    "  \"region\": {\n",
    "    \"center\": {\n",
    "      \"latitude\": 37.767413217936834,\n",
    "      \"longitude\": -122.42820739746094\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "717vOSHHOp7z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.yelp.com/biz/the-duck-inn-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/francos-ristorante-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/kimski-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/mins-noodle-house-%E6%B8%94%E5%AE%B6%E9%87%8D%E5%BA%86%E5%B0%8F%E9%9D%A2-chicago-32?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/shinya-ramen-house-chicago-3?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/do-eat-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/han-202-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/s-k-y-chicago-5?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/the-adelitas-mexican-restaurant-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/stix-n-brix-wood-fired-pizza-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/zaytune-mediterranean-grill-chicago-4?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/gios-cafe-and-deli-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/qing-xiang-yuan-dumplings-chicago-3?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/mccb-chicago-%E6%97%B6%E5%B0%9A%E9%A3%9F%E8%B0%B1-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/taipei-cafe-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/e-ramen-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/apolonia-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/potsticker-house-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/el-ideas-chicago-2?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw',\n",
       "  'https://www.yelp.com/biz/pancho-pistolas-chicago?adjust_creative=jDUfPTzxc7JGj_CVQD9oSw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=jDUfPTzxc7JGj_CVQD9oSw']]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_api_response(data):\n",
    "    \"\"\"\n",
    "    Parse Yelp API results to extract restaurant URLs.\n",
    "\n",
    "    Args:\n",
    "        data (string): String of properly formatted JSON.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of URLs as strings from the input JSON.\n",
    "    \"\"\"\n",
    "\n",
    "#     # [YOUR CODE HERE]\n",
    "\n",
    "    urlList = []\n",
    "    \n",
    "    urlData = data[\"businesses\"]\n",
    "    urlList.append([x[\"url\"] for x in urlData])\n",
    "\n",
    "#     # Printing the extracted URLs\n",
    "#     for url in urls:\n",
    "#         print(\"URL:\", url)\n",
    "    \n",
    "    \n",
    "    return urlList\n",
    "\n",
    "\n",
    "# test your code here to receive credit\n",
    "url, headers, url_params = location_search_params(api_key, \"Bridgeport, Chicago, IL\", offset=0)\n",
    "response_text = api_get_request(url, headers, url_params)\n",
    "parse_api_response(response_text)\n",
    "# ['https://www.yelp.com/biz/nana-chicago?adjust_creative=ioqEYAcUhZO272qCIvxcVA&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=ioqEYAcUhZO272qCIvxcVA',\n",
    "#  'https://www.yelp.com/biz/bridgeport-coffee-chicago-4?adjust_creative=ioqEYAcUhZO272qCIvxcVA&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=ioqEYAcUhZO272qCIvxcVA',\n",
    "# ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf_JIjeUOp7z"
   },
   "source": [
    "As we can see, JSON is quite trivial to parse (which is not the case with HTML as we will see in a second) and work with programmatically. This is why it is one of the most ubiquitous data serialization formats (especially for ReSTful APIs) and a huge benefit of working with a well defined API if one exists. But APIs do not always exists or provide the data we might need, and as a last resort we can always scrape web pages..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUufOxjCOp7z"
   },
   "source": [
    "### Working with Web Pages (and HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poD1suX9Op70"
   },
   "source": [
    "Think of APIs as similar to accessing an application's database itself (something you can interactively query and receive structured data back). But the results are usually in a somewhat raw form with no formatting or visual representation (like the results from a database query). This is a benefit _AND_ a drawback depending on the end use case. For data science and _programatic_ analysis this raw form is quite ideal, but for an end user requesting information from a _graphical interface_ (like a web browser) this is very far from ideal since it takes some cognitive overhead to interpret the raw information. And vice versa, if we have HTML it is quite easy for a human to visually interpret it, but to try to perform some type of programmatic analysis we first need to parse the HTML into a more structured form.\n",
    "\n",
    "> As a general rule of thumb, if the data you need can be accessed or retrieved in a structured form (either from a bulk download or API) prefer that first. But if the data you want (and need) is not as in our case we need to resort to alternative (messier) means.\n",
    "\n",
    "Going back to the \"hello world\" example of question 2.1 with the NYT, we will do something similar to retrieve the HTML of the Yelp site itself (rather than going through the API programmatically) as text.\n",
    "> However, we will use saved HTML pages to reduce excessive traffic to the Yelp website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG1vFsi6Op70"
   },
   "source": [
    "### Question 2.5: Parse a Yelp restaurant Page **(10%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WP0spn8Op70"
   },
   "source": [
    "Using `BeautifulSoup`, parse the HTML of a single Yelp restaurant page to extract the reviews in a structured form as well as the URL to the next page of reviews (or `None` if it is the last page). Fill in following function stubs to parse a single page of reviews and return:\n",
    "* the reviews as a structured Python dictionary\n",
    "* the HTML element containing the link/url for the next page of reviews (or None).\n",
    "\n",
    "For each review be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'author': str\n",
    "    'rating': float\n",
    "    'date': str ('yyyy-mm-dd')\n",
    "    'description': str\n",
    "}\n",
    "\n",
    "{\n",
    "    'author': 'Topsy Kretts'\n",
    "    'rating': 4.7\n",
    "    'date': '2016-01-23'\n",
    "    'description': \"Wonderful!\"\n",
    "}\n",
    "```\n",
    "\n",
    "There can be issues with Beautiful Soup using various parsers, for maximum compatibility (and fewest errors) initialize the library with the default (and Python standard library parser): `BeautifulSoup(markup, \"html.parser\")`.\n",
    "\n",
    "Most of the function has been provided to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "_8tGqNVgOp70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=245\n"
     ]
    }
   ],
   "source": [
    "url_lookup = {\n",
    "\"https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=225\":\"parse_page_test1.html\",\n",
    "\"https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=245\":\"parse_page_test2.html\"\n",
    "}\n",
    "\n",
    "def html_fetcher(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "    Args:\n",
    "        url (string):\n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    html_file = url_lookup.get(url)\n",
    "    with open(html_file, 'rb') as file:\n",
    "        html_text = file.read()\n",
    "        return 200, html_text\n",
    "\n",
    "\n",
    "def parse_page(html):\n",
    "    \"\"\"\n",
    "    Parse the reviews on a single page of a restaurant.\n",
    "\n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Yelp restaurant\n",
    "\n",
    "    Returns:\n",
    "        tuple(list, string): a tuple of two elements\n",
    "            first element: list of dictionaries corresponding to the extracted review information\n",
    "            second element: URL for the next page of reviews (or None if it is the last page)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    url_next = soup.find('link',rel='next')\n",
    "    if url_next:\n",
    "        url_next = url_next.get('href')\n",
    "    else:\n",
    "        url_next = None\n",
    "\n",
    "    reviews = soup.find_all('div', itemprop=\"review\")\n",
    "    reviews_list = []\n",
    "    # HINT: print reviews to see what http tag to extract\n",
    "    # [YOUR CODE HERE]\n",
    "    \n",
    "    for review in reviews:\n",
    "        reviewDict = {\n",
    "            'author': str(review.find('meta', itemprop=\"author\")[\"content\"]),\n",
    "            'rating': float(review.find('meta', itemprop=\"ratingValue\")[\"content\"]),\n",
    "            'date': review.find('meta', itemprop=\"datePublished\")[\"content\"],\n",
    "            'description': str(review.find('p', itemprop=\"description\").get_text(strip=True))\n",
    "        }\n",
    "        reviews_list.append(reviewDict)\n",
    "\n",
    "    return reviews_list, url_next\n",
    "\n",
    "# Test your implementation here to receive credit.\n",
    "code, html = html_fetcher(\"https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=225\")\n",
    "reviews_list, url_next = parse_page(html)\n",
    "print(len(reviews_list)) # 20\n",
    "print(url_next) #https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG9bueg9Op71"
   },
   "source": [
    "### Question 2.6: Extract all Yelp reviews for a Single Restaurant ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGbCCAvJOp71"
   },
   "source": [
    "So now that we have parsed a single page, and figured out a method to go from one page to the next we are ready to combine these two techniques and actually crawl through web pages!\n",
    "\n",
    "Using the provided `html_fetcher` (for a real use-case you would use `requests`), programmatically retrieve __ALL__ of the reviews for a __single__ restaurant (provided as a parameter). Just like the API was paginated, the HTML paginates its reviews (it would be a very long web page to show 300 reviews on a single page) and to get all the reviews you will need to parse and traverse the HTML. As input your function will receive a URL corresponding to a Yelp restaurant. As output return a list of dictionaries (structured the same as question 2.5) containing the relevant information from the reviews. You can use `parse_page()` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "C8i3Rj2eOp71"
   },
   "outputs": [],
   "source": [
    "def extract_reviews(url, html_fetcher):\n",
    "    \"\"\"\n",
    "    Retrieve ALL of the reviews for a single restaurant on Yelp.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): Yelp URL corresponding to the restaurant of interest.\n",
    "        html_fetcher (function): A function that takes url and returns html status code and content\n",
    "\n",
    "    Returns:\n",
    "        reviews (list): list of dictionaries containing extracted review information\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    # [YOUR CODE HERE]\n",
    "    # HINT: Use function `parse_page(html)` multiple times until no next page exists\n",
    "    #while loop since we dont know how many pages we gotta do\n",
    "    while url:\n",
    "        errorCode,html = html_fetcher(url)\n",
    "        if code != 200:\n",
    "            return reviews\n",
    "        \n",
    "        review,url = parse_page(html)\n",
    "        reviews.extend(review)\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uxeoq-YOp71"
   },
   "source": [
    "You can test your function with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqUTkMr1Op72",
    "outputId": "2d3d41f4-9065-45c5-93f3-f4082b9fdd6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "{'author': 'Jason S.', 'rating': 5.0, 'date': '2016-05-02', 'description': \"This was one of my favorite food trucks but as of last fall they've opened a brick and mortar restaurant in the Pilsen neighborhood...the perfect success story of how a person can start out with a food truck and grow their business into a restaurant. The food is always delicious and the service is great!\"}\n"
     ]
    }
   ],
   "source": [
    "# test your function here to receive credit.\n",
    "data = extract_reviews('https://www.yelp.com/biz/the-jibarito-stop-chicago-2?start=225', html_fetcher=html_fetcher)\n",
    "print(len(data))\n",
    "# 35\n",
    "print(data[0])\n",
    "# {'author': 'Jason S.', 'rating': 5.0, 'date': '2016-05-02', 'description': \"This was one of my favorite food trucks ...\"}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1cd19471928a1bd1d711eeee745b6e15cf888a1d547a12676dd6c4d7e50130"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
